{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCywmlF+OynzZ/Ga26ryp9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitpatelsitapur/my-py-torch-journey/blob/main/Pytorch_Dataset_and_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problems** when We Don't use Dataset and Data Loader class\n",
        "- No batch management\n",
        "- Sequential , no parallelization\n",
        "- No Shuffling and sampling\n",
        "\n",
        "## **Solution**\n",
        "- **Dataset** class - a `Blueprint` for loading and returning data.\n",
        "  - `__init__()`- structure of data/batches\n",
        "  - `__len__()`- no of batches/samples\n",
        "  - `__getitem__(index)`- return data at given index\n",
        "\n",
        "- **DataLoader** class- handles batching, shuffling and parallel loading\n",
        "  - `dataset`(mandatory)\n",
        "  - `batch_size`\n",
        "  - `shuffle=True` for suffling data with indices\n",
        "  - `num_workers` for parallelization\n",
        "  - `pin_memory` for gpu transfer with high speed from memeory\n",
        "  - `collate_fn` for collect and combining data into batches, helpful in *padding of uneven size data*\n",
        "  - `drop_last` whether drop incomplete batch or not\n",
        "  - `sampler` for custom suffling\n"
      ],
      "metadata": {
        "id": "QaSaFEHeWX48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7awnyXEkZN_T",
        "outputId": "f8154f55-431a-470f-c76e-d93937c235f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVx_HNHgPDjq"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=make_classification(n_samples=20,n_features=4,n_classes=2,random_state=42)\n",
        "print(f\"X.shape = {X.shape}, y.shape = {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E20jOFF0ZHiT",
        "outputId": "26699497-0104-494d-95dd-d429f6383d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape = (20, 4), y.shape = (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor(X,dtype=torch.float32)\n",
        "y=torch.tensor(y,dtype=torch.float32)\n",
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwbwH03VZcRo",
        "outputId": "f55f647f-cc7a-4140-e6b6-98635aadab39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.3721,  1.2558,  0.3967, -0.5853],\n",
              "         [ 1.3306, -0.2071,  1.1705, -0.9834],\n",
              "         [ 0.5591,  0.9470,  1.1168, -1.1558],\n",
              "         [ 0.8922, -0.7074,  0.4412, -0.2511],\n",
              "         [ 0.4750,  1.1577,  1.1623, -1.2356],\n",
              "         [ 0.9135,  1.4545,  1.7687, -1.8219],\n",
              "         [-0.1135, -0.8750, -0.6394,  0.7249],\n",
              "         [-0.8606,  1.4704,  0.0507, -0.3238],\n",
              "         [ 0.6601,  2.0757,  1.8973, -2.0523],\n",
              "         [ 1.3973,  0.0405,  1.3852, -1.2181],\n",
              "         [-0.0456, -1.4569, -0.9250,  1.0851],\n",
              "         [-1.4088, -0.2828, -1.5427,  1.4018],\n",
              "         [ 0.7728,  1.9959,  1.9589, -2.0910],\n",
              "         [ 2.3675,  0.6220,  2.6814, -2.4612],\n",
              "         [ 1.0926, -0.0154,  1.0546, -0.9186],\n",
              "         [-0.8299, -0.9640, -1.3908,  1.3985],\n",
              "         [-0.8359, -0.6550, -1.2098,  1.1816],\n",
              "         [-1.3137, -1.4257, -2.1410,  2.1417],\n",
              "         [-0.7697,  1.9632,  0.4370, -0.7549],\n",
              "         [-0.6568,  1.6113,  0.3343, -0.5983]]),\n",
              " tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
              "         0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Using Pytorch Dataset and DataLoader***"
      ],
      "metadata": {
        "id": "umsK99c-Zzsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "dataset=custom_dataset(X,y)"
      ],
      "metadata": {
        "id": "I_s9xL60ZzRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2PFzfBiZpDG",
        "outputId": "02068af3-6f37-4982-ed0f-0a31b2ac67d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.3721,  1.2558,  0.3967, -0.5853]), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader=DataLoader(dataset=dataset,batch_size=5,shuffle=True)\n",
        "\n",
        "for batch_X,batch_y in dataloader:\n",
        "  print(batch_X)\n",
        "  print(batch_y)\n",
        "  print(f\"batch_X.shape = {batch_X.shape}, batch_y.shape = {batch_y.shape}\")\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh-7E59saiJy",
        "outputId": "012208e2-e211-4788-a360-a1788bd3989a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5591,  0.9470,  1.1168, -1.1558],\n",
            "        [ 0.4750,  1.1577,  1.1623, -1.2356],\n",
            "        [-0.0456, -1.4569, -0.9250,  1.0851],\n",
            "        [-0.8299, -0.9640, -1.3908,  1.3985],\n",
            "        [ 0.6601,  2.0757,  1.8973, -2.0523]])\n",
            "tensor([0., 0., 1., 0., 1.])\n",
            "batch_X.shape = torch.Size([5, 4]), batch_y.shape = torch.Size([5])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.6568,  1.6113,  0.3343, -0.5983],\n",
            "        [ 1.3973,  0.0405,  1.3852, -1.2181],\n",
            "        [-0.1135, -0.8750, -0.6394,  0.7249],\n",
            "        [-1.3137, -1.4257, -2.1410,  2.1417],\n",
            "        [ 2.3675,  0.6220,  2.6814, -2.4612]])\n",
            "tensor([0., 1., 1., 0., 1.])\n",
            "batch_X.shape = torch.Size([5, 4]), batch_y.shape = torch.Size([5])\n",
            "--------------------------------------------------\n",
            "tensor([[ 0.9135,  1.4545,  1.7687, -1.8219],\n",
            "        [ 0.7728,  1.9959,  1.9589, -2.0910],\n",
            "        [ 1.3306, -0.2071,  1.1705, -0.9834],\n",
            "        [-1.4088, -0.2828, -1.5427,  1.4018],\n",
            "        [-0.8606,  1.4704,  0.0507, -0.3238]])\n",
            "tensor([1., 1., 1., 0., 0.])\n",
            "batch_X.shape = torch.Size([5, 4]), batch_y.shape = torch.Size([5])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.7697,  1.9632,  0.4370, -0.7549],\n",
            "        [ 0.8922, -0.7074,  0.4412, -0.2511],\n",
            "        [ 1.0926, -0.0154,  1.0546, -0.9186],\n",
            "        [-0.8359, -0.6550, -1.2098,  1.1816],\n",
            "        [-0.3721,  1.2558,  0.3967, -0.5853]])\n",
            "tensor([0., 1., 1., 0., 0.])\n",
            "batch_X.shape = torch.Size([5, 4]), batch_y.shape = torch.Size([5])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying it in Breast Cancer Classification data**"
      ],
      "metadata": {
        "id": "DWTgokWrc2t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data preprocessing\n",
        "df=pd.read_csv(\"https://raw.githubusercontent.com/gscdit/Breast-cancer-Detection/refs/heads/master/data.csv\")\n",
        "df.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\n",
        "\n",
        "X=df.drop([\"diagnosis\"],axis=1)\n",
        "y=df[\"diagnosis\"]\n",
        "\n",
        "le=LabelEncoder()\n",
        "y=le.fit_transform(y)\n",
        "\n",
        "sc=StandardScaler()\n",
        "X=sc.fit_transform(X)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))      # hidden layer with ReLU\n",
        "        x = torch.sigmoid(self.fc2(x))   # output with Sigmoid\n",
        "        return x"
      ],
      "metadata": {
        "id": "EtcqGcaDaivx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train_t.shape = {X_train_t.shape}, y_train_t.shape = {y_train_t.shape}\")\n",
        "print(f\"X_test_t.shape = {X_test_t.shape}, y_test_t.shape = {y_test_t.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS0YL1W2eGXT",
        "outputId": "62c71d76-fea7-4f0a-e200-8ebebbf78765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_t.shape = torch.Size([455, 30]), y_train_t.shape = torch.Size([455, 1])\n",
            "X_test_t.shape = torch.Size([114, 30]), y_test_t.shape = torch.Size([114, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define datset and dataloader\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "hBald8sjairV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 16\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size)\n",
        "loss = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "Hd1v80Sse5AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syZir7M6glnb",
        "outputId": "2fd5c784-19b1-4a8c-d636-872c4c03b6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "SimpleNN                                 --\n",
              "├─Linear: 1-1                            496\n",
              "├─Linear: 1-2                            17\n",
              "=================================================================\n",
              "Total params: 513\n",
              "Trainable params: 513\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training (whole dataset each epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        outputs = model(batch_X)\n",
        "        l = loss(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {l.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1TGscf3f9OY",
        "outputId": "652d0527-2881-4e7e-c51a-0fef6ea058e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.8264\n",
            "Epoch [1/50], Loss: 0.7747\n",
            "Epoch [1/50], Loss: 0.7350\n",
            "Epoch [1/50], Loss: 0.7626\n",
            "Epoch [1/50], Loss: 0.6809\n",
            "Epoch [1/50], Loss: 0.8044\n",
            "Epoch [1/50], Loss: 0.6721\n",
            "Epoch [1/50], Loss: 0.6740\n",
            "Epoch [1/50], Loss: 0.7029\n",
            "Epoch [1/50], Loss: 0.7089\n",
            "Epoch [1/50], Loss: 0.6950\n",
            "Epoch [1/50], Loss: 0.6392\n",
            "Epoch [1/50], Loss: 0.6438\n",
            "Epoch [1/50], Loss: 0.7136\n",
            "Epoch [1/50], Loss: 0.5916\n",
            "Epoch [2/50], Loss: 0.6967\n",
            "Epoch [2/50], Loss: 0.6003\n",
            "Epoch [2/50], Loss: 0.6301\n",
            "Epoch [2/50], Loss: 0.6238\n",
            "Epoch [2/50], Loss: 0.6502\n",
            "Epoch [2/50], Loss: 0.6145\n",
            "Epoch [2/50], Loss: 0.6218\n",
            "Epoch [2/50], Loss: 0.5673\n",
            "Epoch [2/50], Loss: 0.5966\n",
            "Epoch [2/50], Loss: 0.5826\n",
            "Epoch [2/50], Loss: 0.5845\n",
            "Epoch [2/50], Loss: 0.5842\n",
            "Epoch [2/50], Loss: 0.6059\n",
            "Epoch [2/50], Loss: 0.5605\n",
            "Epoch [2/50], Loss: 0.5679\n",
            "Epoch [3/50], Loss: 0.5722\n",
            "Epoch [3/50], Loss: 0.5456\n",
            "Epoch [3/50], Loss: 0.5332\n",
            "Epoch [3/50], Loss: 0.5377\n",
            "Epoch [3/50], Loss: 0.5050\n",
            "Epoch [3/50], Loss: 0.5284\n",
            "Epoch [3/50], Loss: 0.5015\n",
            "Epoch [3/50], Loss: 0.5007\n",
            "Epoch [3/50], Loss: 0.5192\n",
            "Epoch [3/50], Loss: 0.5019\n",
            "Epoch [3/50], Loss: 0.5197\n",
            "Epoch [3/50], Loss: 0.5077\n",
            "Epoch [3/50], Loss: 0.5006\n",
            "Epoch [3/50], Loss: 0.4682\n",
            "Epoch [3/50], Loss: 0.4526\n",
            "Epoch [4/50], Loss: 0.4891\n",
            "Epoch [4/50], Loss: 0.4540\n",
            "Epoch [4/50], Loss: 0.4756\n",
            "Epoch [4/50], Loss: 0.4189\n",
            "Epoch [4/50], Loss: 0.4708\n",
            "Epoch [4/50], Loss: 0.4338\n",
            "Epoch [4/50], Loss: 0.4467\n",
            "Epoch [4/50], Loss: 0.4396\n",
            "Epoch [4/50], Loss: 0.4292\n",
            "Epoch [4/50], Loss: 0.3756\n",
            "Epoch [4/50], Loss: 0.4520\n",
            "Epoch [4/50], Loss: 0.4917\n",
            "Epoch [4/50], Loss: 0.3639\n",
            "Epoch [4/50], Loss: 0.3628\n",
            "Epoch [4/50], Loss: 0.4235\n",
            "Epoch [5/50], Loss: 0.4171\n",
            "Epoch [5/50], Loss: 0.3425\n",
            "Epoch [5/50], Loss: 0.4029\n",
            "Epoch [5/50], Loss: 0.3203\n",
            "Epoch [5/50], Loss: 0.3890\n",
            "Epoch [5/50], Loss: 0.3752\n",
            "Epoch [5/50], Loss: 0.3686\n",
            "Epoch [5/50], Loss: 0.3705\n",
            "Epoch [5/50], Loss: 0.4080\n",
            "Epoch [5/50], Loss: 0.3686\n",
            "Epoch [5/50], Loss: 0.3980\n",
            "Epoch [5/50], Loss: 0.3256\n",
            "Epoch [5/50], Loss: 0.3433\n",
            "Epoch [5/50], Loss: 0.3370\n",
            "Epoch [5/50], Loss: 0.3540\n",
            "Epoch [6/50], Loss: 0.3105\n",
            "Epoch [6/50], Loss: 0.4165\n",
            "Epoch [6/50], Loss: 0.3162\n",
            "Epoch [6/50], Loss: 0.3409\n",
            "Epoch [6/50], Loss: 0.2753\n",
            "Epoch [6/50], Loss: 0.2972\n",
            "Epoch [6/50], Loss: 0.3502\n",
            "Epoch [6/50], Loss: 0.2599\n",
            "Epoch [6/50], Loss: 0.3162\n",
            "Epoch [6/50], Loss: 0.3159\n",
            "Epoch [6/50], Loss: 0.3129\n",
            "Epoch [6/50], Loss: 0.3495\n",
            "Epoch [6/50], Loss: 0.2918\n",
            "Epoch [6/50], Loss: 0.2412\n",
            "Epoch [6/50], Loss: 0.2894\n",
            "Epoch [7/50], Loss: 0.2473\n",
            "Epoch [7/50], Loss: 0.2660\n",
            "Epoch [7/50], Loss: 0.3318\n",
            "Epoch [7/50], Loss: 0.3224\n",
            "Epoch [7/50], Loss: 0.2539\n",
            "Epoch [7/50], Loss: 0.2913\n",
            "Epoch [7/50], Loss: 0.2812\n",
            "Epoch [7/50], Loss: 0.2710\n",
            "Epoch [7/50], Loss: 0.3095\n",
            "Epoch [7/50], Loss: 0.3021\n",
            "Epoch [7/50], Loss: 0.1983\n",
            "Epoch [7/50], Loss: 0.2062\n",
            "Epoch [7/50], Loss: 0.2831\n",
            "Epoch [7/50], Loss: 0.2098\n",
            "Epoch [7/50], Loss: 0.2567\n",
            "Epoch [8/50], Loss: 0.2309\n",
            "Epoch [8/50], Loss: 0.2022\n",
            "Epoch [8/50], Loss: 0.2919\n",
            "Epoch [8/50], Loss: 0.2439\n",
            "Epoch [8/50], Loss: 0.2139\n",
            "Epoch [8/50], Loss: 0.2673\n",
            "Epoch [8/50], Loss: 0.1835\n",
            "Epoch [8/50], Loss: 0.2629\n",
            "Epoch [8/50], Loss: 0.2047\n",
            "Epoch [8/50], Loss: 0.2464\n",
            "Epoch [8/50], Loss: 0.2406\n",
            "Epoch [8/50], Loss: 0.2722\n",
            "Epoch [8/50], Loss: 0.2194\n",
            "Epoch [8/50], Loss: 0.2390\n",
            "Epoch [8/50], Loss: 0.1225\n",
            "Epoch [9/50], Loss: 0.2457\n",
            "Epoch [9/50], Loss: 0.1995\n",
            "Epoch [9/50], Loss: 0.2259\n",
            "Epoch [9/50], Loss: 0.2946\n",
            "Epoch [9/50], Loss: 0.1618\n",
            "Epoch [9/50], Loss: 0.1791\n",
            "Epoch [9/50], Loss: 0.2196\n",
            "Epoch [9/50], Loss: 0.2161\n",
            "Epoch [9/50], Loss: 0.2428\n",
            "Epoch [9/50], Loss: 0.2144\n",
            "Epoch [9/50], Loss: 0.2118\n",
            "Epoch [9/50], Loss: 0.1664\n",
            "Epoch [9/50], Loss: 0.1981\n",
            "Epoch [9/50], Loss: 0.1438\n",
            "Epoch [9/50], Loss: 0.1268\n",
            "Epoch [10/50], Loss: 0.2025\n",
            "Epoch [10/50], Loss: 0.2390\n",
            "Epoch [10/50], Loss: 0.1288\n",
            "Epoch [10/50], Loss: 0.1344\n",
            "Epoch [10/50], Loss: 0.1674\n",
            "Epoch [10/50], Loss: 0.2037\n",
            "Epoch [10/50], Loss: 0.2547\n",
            "Epoch [10/50], Loss: 0.1962\n",
            "Epoch [10/50], Loss: 0.1727\n",
            "Epoch [10/50], Loss: 0.1800\n",
            "Epoch [10/50], Loss: 0.1756\n",
            "Epoch [10/50], Loss: 0.2076\n",
            "Epoch [10/50], Loss: 0.1518\n",
            "Epoch [10/50], Loss: 0.2030\n",
            "Epoch [10/50], Loss: 0.1060\n",
            "Epoch [11/50], Loss: 0.2048\n",
            "Epoch [11/50], Loss: 0.1704\n",
            "Epoch [11/50], Loss: 0.2532\n",
            "Epoch [11/50], Loss: 0.1689\n",
            "Epoch [11/50], Loss: 0.1655\n",
            "Epoch [11/50], Loss: 0.1728\n",
            "Epoch [11/50], Loss: 0.1095\n",
            "Epoch [11/50], Loss: 0.1106\n",
            "Epoch [11/50], Loss: 0.1712\n",
            "Epoch [11/50], Loss: 0.1730\n",
            "Epoch [11/50], Loss: 0.1088\n",
            "Epoch [11/50], Loss: 0.2099\n",
            "Epoch [11/50], Loss: 0.1928\n",
            "Epoch [11/50], Loss: 0.1365\n",
            "Epoch [11/50], Loss: 0.1583\n",
            "Epoch [12/50], Loss: 0.1297\n",
            "Epoch [12/50], Loss: 0.1904\n",
            "Epoch [12/50], Loss: 0.1383\n",
            "Epoch [12/50], Loss: 0.1716\n",
            "Epoch [12/50], Loss: 0.1271\n",
            "Epoch [12/50], Loss: 0.1885\n",
            "Epoch [12/50], Loss: 0.1300\n",
            "Epoch [12/50], Loss: 0.1687\n",
            "Epoch [12/50], Loss: 0.1240\n",
            "Epoch [12/50], Loss: 0.1772\n",
            "Epoch [12/50], Loss: 0.1956\n",
            "Epoch [12/50], Loss: 0.1418\n",
            "Epoch [12/50], Loss: 0.1283\n",
            "Epoch [12/50], Loss: 0.1445\n",
            "Epoch [12/50], Loss: 0.0898\n",
            "Epoch [13/50], Loss: 0.1044\n",
            "Epoch [13/50], Loss: 0.1829\n",
            "Epoch [13/50], Loss: 0.1378\n",
            "Epoch [13/50], Loss: 0.0701\n",
            "Epoch [13/50], Loss: 0.1726\n",
            "Epoch [13/50], Loss: 0.1356\n",
            "Epoch [13/50], Loss: 0.1670\n",
            "Epoch [13/50], Loss: 0.1473\n",
            "Epoch [13/50], Loss: 0.1899\n",
            "Epoch [13/50], Loss: 0.1477\n",
            "Epoch [13/50], Loss: 0.1640\n",
            "Epoch [13/50], Loss: 0.1652\n",
            "Epoch [13/50], Loss: 0.0992\n",
            "Epoch [13/50], Loss: 0.0935\n",
            "Epoch [13/50], Loss: 0.1143\n",
            "Epoch [14/50], Loss: 0.2419\n",
            "Epoch [14/50], Loss: 0.1279\n",
            "Epoch [14/50], Loss: 0.0527\n",
            "Epoch [14/50], Loss: 0.1290\n",
            "Epoch [14/50], Loss: 0.1033\n",
            "Epoch [14/50], Loss: 0.0884\n",
            "Epoch [14/50], Loss: 0.1117\n",
            "Epoch [14/50], Loss: 0.1158\n",
            "Epoch [14/50], Loss: 0.1558\n",
            "Epoch [14/50], Loss: 0.1436\n",
            "Epoch [14/50], Loss: 0.1436\n",
            "Epoch [14/50], Loss: 0.0748\n",
            "Epoch [14/50], Loss: 0.1704\n",
            "Epoch [14/50], Loss: 0.0979\n",
            "Epoch [14/50], Loss: 0.4896\n",
            "Epoch [15/50], Loss: 0.1672\n",
            "Epoch [15/50], Loss: 0.0921\n",
            "Epoch [15/50], Loss: 0.0655\n",
            "Epoch [15/50], Loss: 0.0606\n",
            "Epoch [15/50], Loss: 0.0858\n",
            "Epoch [15/50], Loss: 0.0941\n",
            "Epoch [15/50], Loss: 0.1899\n",
            "Epoch [15/50], Loss: 0.1227\n",
            "Epoch [15/50], Loss: 0.1521\n",
            "Epoch [15/50], Loss: 0.1120\n",
            "Epoch [15/50], Loss: 0.1233\n",
            "Epoch [15/50], Loss: 0.1279\n",
            "Epoch [15/50], Loss: 0.0978\n",
            "Epoch [15/50], Loss: 0.2525\n",
            "Epoch [15/50], Loss: 0.0377\n",
            "Epoch [16/50], Loss: 0.1909\n",
            "Epoch [16/50], Loss: 0.1789\n",
            "Epoch [16/50], Loss: 0.1789\n",
            "Epoch [16/50], Loss: 0.1604\n",
            "Epoch [16/50], Loss: 0.0528\n",
            "Epoch [16/50], Loss: 0.0537\n",
            "Epoch [16/50], Loss: 0.0555\n",
            "Epoch [16/50], Loss: 0.0885\n",
            "Epoch [16/50], Loss: 0.1305\n",
            "Epoch [16/50], Loss: 0.1509\n",
            "Epoch [16/50], Loss: 0.1044\n",
            "Epoch [16/50], Loss: 0.0678\n",
            "Epoch [16/50], Loss: 0.1159\n",
            "Epoch [16/50], Loss: 0.1134\n",
            "Epoch [16/50], Loss: 0.0357\n",
            "Epoch [17/50], Loss: 0.1236\n",
            "Epoch [17/50], Loss: 0.0899\n",
            "Epoch [17/50], Loss: 0.1563\n",
            "Epoch [17/50], Loss: 0.0689\n",
            "Epoch [17/50], Loss: 0.1232\n",
            "Epoch [17/50], Loss: 0.0716\n",
            "Epoch [17/50], Loss: 0.1112\n",
            "Epoch [17/50], Loss: 0.1518\n",
            "Epoch [17/50], Loss: 0.1373\n",
            "Epoch [17/50], Loss: 0.1121\n",
            "Epoch [17/50], Loss: 0.0895\n",
            "Epoch [17/50], Loss: 0.0948\n",
            "Epoch [17/50], Loss: 0.1211\n",
            "Epoch [17/50], Loss: 0.0950\n",
            "Epoch [17/50], Loss: 0.0841\n",
            "Epoch [18/50], Loss: 0.1733\n",
            "Epoch [18/50], Loss: 0.1888\n",
            "Epoch [18/50], Loss: 0.1042\n",
            "Epoch [18/50], Loss: 0.1540\n",
            "Epoch [18/50], Loss: 0.0561\n",
            "Epoch [18/50], Loss: 0.1646\n",
            "Epoch [18/50], Loss: 0.1040\n",
            "Epoch [18/50], Loss: 0.0965\n",
            "Epoch [18/50], Loss: 0.0869\n",
            "Epoch [18/50], Loss: 0.0727\n",
            "Epoch [18/50], Loss: 0.0563\n",
            "Epoch [18/50], Loss: 0.0462\n",
            "Epoch [18/50], Loss: 0.0980\n",
            "Epoch [18/50], Loss: 0.0701\n",
            "Epoch [18/50], Loss: 0.0650\n",
            "Epoch [19/50], Loss: 0.2438\n",
            "Epoch [19/50], Loss: 0.1755\n",
            "Epoch [19/50], Loss: 0.0682\n",
            "Epoch [19/50], Loss: 0.0566\n",
            "Epoch [19/50], Loss: 0.0840\n",
            "Epoch [19/50], Loss: 0.0692\n",
            "Epoch [19/50], Loss: 0.0487\n",
            "Epoch [19/50], Loss: 0.0826\n",
            "Epoch [19/50], Loss: 0.0773\n",
            "Epoch [19/50], Loss: 0.1106\n",
            "Epoch [19/50], Loss: 0.1112\n",
            "Epoch [19/50], Loss: 0.0470\n",
            "Epoch [19/50], Loss: 0.0851\n",
            "Epoch [19/50], Loss: 0.1531\n",
            "Epoch [19/50], Loss: 0.0278\n",
            "Epoch [20/50], Loss: 0.1274\n",
            "Epoch [20/50], Loss: 0.1141\n",
            "Epoch [20/50], Loss: 0.1355\n",
            "Epoch [20/50], Loss: 0.0786\n",
            "Epoch [20/50], Loss: 0.0977\n",
            "Epoch [20/50], Loss: 0.0533\n",
            "Epoch [20/50], Loss: 0.0714\n",
            "Epoch [20/50], Loss: 0.1104\n",
            "Epoch [20/50], Loss: 0.0669\n",
            "Epoch [20/50], Loss: 0.0416\n",
            "Epoch [20/50], Loss: 0.0860\n",
            "Epoch [20/50], Loss: 0.1175\n",
            "Epoch [20/50], Loss: 0.1723\n",
            "Epoch [20/50], Loss: 0.0841\n",
            "Epoch [20/50], Loss: 0.0142\n",
            "Epoch [21/50], Loss: 0.0567\n",
            "Epoch [21/50], Loss: 0.1107\n",
            "Epoch [21/50], Loss: 0.0667\n",
            "Epoch [21/50], Loss: 0.1033\n",
            "Epoch [21/50], Loss: 0.0788\n",
            "Epoch [21/50], Loss: 0.0385\n",
            "Epoch [21/50], Loss: 0.0592\n",
            "Epoch [21/50], Loss: 0.2278\n",
            "Epoch [21/50], Loss: 0.0571\n",
            "Epoch [21/50], Loss: 0.0976\n",
            "Epoch [21/50], Loss: 0.0580\n",
            "Epoch [21/50], Loss: 0.0337\n",
            "Epoch [21/50], Loss: 0.2039\n",
            "Epoch [21/50], Loss: 0.1103\n",
            "Epoch [21/50], Loss: 0.0185\n",
            "Epoch [22/50], Loss: 0.0589\n",
            "Epoch [22/50], Loss: 0.0576\n",
            "Epoch [22/50], Loss: 0.0901\n",
            "Epoch [22/50], Loss: 0.0457\n",
            "Epoch [22/50], Loss: 0.1129\n",
            "Epoch [22/50], Loss: 0.1163\n",
            "Epoch [22/50], Loss: 0.1289\n",
            "Epoch [22/50], Loss: 0.0517\n",
            "Epoch [22/50], Loss: 0.1842\n",
            "Epoch [22/50], Loss: 0.0480\n",
            "Epoch [22/50], Loss: 0.1491\n",
            "Epoch [22/50], Loss: 0.0422\n",
            "Epoch [22/50], Loss: 0.0649\n",
            "Epoch [22/50], Loss: 0.0805\n",
            "Epoch [22/50], Loss: 0.1374\n",
            "Epoch [23/50], Loss: 0.1450\n",
            "Epoch [23/50], Loss: 0.0545\n",
            "Epoch [23/50], Loss: 0.0507\n",
            "Epoch [23/50], Loss: 0.0669\n",
            "Epoch [23/50], Loss: 0.0435\n",
            "Epoch [23/50], Loss: 0.2318\n",
            "Epoch [23/50], Loss: 0.0413\n",
            "Epoch [23/50], Loss: 0.1507\n",
            "Epoch [23/50], Loss: 0.0903\n",
            "Epoch [23/50], Loss: 0.0430\n",
            "Epoch [23/50], Loss: 0.0605\n",
            "Epoch [23/50], Loss: 0.0713\n",
            "Epoch [23/50], Loss: 0.0781\n",
            "Epoch [23/50], Loss: 0.0877\n",
            "Epoch [23/50], Loss: 0.0241\n",
            "Epoch [24/50], Loss: 0.0974\n",
            "Epoch [24/50], Loss: 0.0251\n",
            "Epoch [24/50], Loss: 0.0851\n",
            "Epoch [24/50], Loss: 0.0690\n",
            "Epoch [24/50], Loss: 0.0453\n",
            "Epoch [24/50], Loss: 0.0443\n",
            "Epoch [24/50], Loss: 0.0530\n",
            "Epoch [24/50], Loss: 0.0920\n",
            "Epoch [24/50], Loss: 0.0842\n",
            "Epoch [24/50], Loss: 0.2490\n",
            "Epoch [24/50], Loss: 0.1271\n",
            "Epoch [24/50], Loss: 0.1067\n",
            "Epoch [24/50], Loss: 0.0389\n",
            "Epoch [24/50], Loss: 0.0572\n",
            "Epoch [24/50], Loss: 0.0426\n",
            "Epoch [25/50], Loss: 0.0476\n",
            "Epoch [25/50], Loss: 0.0930\n",
            "Epoch [25/50], Loss: 0.1791\n",
            "Epoch [25/50], Loss: 0.1091\n",
            "Epoch [25/50], Loss: 0.0574\n",
            "Epoch [25/50], Loss: 0.1465\n",
            "Epoch [25/50], Loss: 0.1488\n",
            "Epoch [25/50], Loss: 0.0161\n",
            "Epoch [25/50], Loss: 0.0472\n",
            "Epoch [25/50], Loss: 0.1157\n",
            "Epoch [25/50], Loss: 0.0260\n",
            "Epoch [25/50], Loss: 0.0465\n",
            "Epoch [25/50], Loss: 0.0451\n",
            "Epoch [25/50], Loss: 0.0681\n",
            "Epoch [25/50], Loss: 0.0114\n",
            "Epoch [26/50], Loss: 0.0515\n",
            "Epoch [26/50], Loss: 0.0503\n",
            "Epoch [26/50], Loss: 0.1211\n",
            "Epoch [26/50], Loss: 0.0456\n",
            "Epoch [26/50], Loss: 0.0447\n",
            "Epoch [26/50], Loss: 0.1108\n",
            "Epoch [26/50], Loss: 0.0413\n",
            "Epoch [26/50], Loss: 0.0769\n",
            "Epoch [26/50], Loss: 0.0725\n",
            "Epoch [26/50], Loss: 0.0688\n",
            "Epoch [26/50], Loss: 0.0783\n",
            "Epoch [26/50], Loss: 0.0370\n",
            "Epoch [26/50], Loss: 0.2767\n",
            "Epoch [26/50], Loss: 0.0404\n",
            "Epoch [26/50], Loss: 0.0191\n",
            "Epoch [27/50], Loss: 0.0343\n",
            "Epoch [27/50], Loss: 0.0515\n",
            "Epoch [27/50], Loss: 0.0732\n",
            "Epoch [27/50], Loss: 0.1248\n",
            "Epoch [27/50], Loss: 0.0373\n",
            "Epoch [27/50], Loss: 0.1774\n",
            "Epoch [27/50], Loss: 0.0293\n",
            "Epoch [27/50], Loss: 0.0453\n",
            "Epoch [27/50], Loss: 0.0916\n",
            "Epoch [27/50], Loss: 0.0330\n",
            "Epoch [27/50], Loss: 0.0829\n",
            "Epoch [27/50], Loss: 0.1082\n",
            "Epoch [27/50], Loss: 0.1000\n",
            "Epoch [27/50], Loss: 0.0449\n",
            "Epoch [27/50], Loss: 0.2667\n",
            "Epoch [28/50], Loss: 0.0645\n",
            "Epoch [28/50], Loss: 0.0395\n",
            "Epoch [28/50], Loss: 0.2398\n",
            "Epoch [28/50], Loss: 0.1324\n",
            "Epoch [28/50], Loss: 0.0616\n",
            "Epoch [28/50], Loss: 0.0875\n",
            "Epoch [28/50], Loss: 0.0340\n",
            "Epoch [28/50], Loss: 0.0447\n",
            "Epoch [28/50], Loss: 0.0507\n",
            "Epoch [28/50], Loss: 0.0457\n",
            "Epoch [28/50], Loss: 0.0366\n",
            "Epoch [28/50], Loss: 0.0698\n",
            "Epoch [28/50], Loss: 0.0572\n",
            "Epoch [28/50], Loss: 0.1038\n",
            "Epoch [28/50], Loss: 0.0059\n",
            "Epoch [29/50], Loss: 0.1035\n",
            "Epoch [29/50], Loss: 0.0513\n",
            "Epoch [29/50], Loss: 0.0609\n",
            "Epoch [29/50], Loss: 0.0356\n",
            "Epoch [29/50], Loss: 0.0833\n",
            "Epoch [29/50], Loss: 0.1704\n",
            "Epoch [29/50], Loss: 0.0999\n",
            "Epoch [29/50], Loss: 0.0375\n",
            "Epoch [29/50], Loss: 0.1054\n",
            "Epoch [29/50], Loss: 0.0364\n",
            "Epoch [29/50], Loss: 0.0345\n",
            "Epoch [29/50], Loss: 0.0583\n",
            "Epoch [29/50], Loss: 0.1090\n",
            "Epoch [29/50], Loss: 0.0547\n",
            "Epoch [29/50], Loss: 0.0220\n",
            "Epoch [30/50], Loss: 0.0718\n",
            "Epoch [30/50], Loss: 0.0664\n",
            "Epoch [30/50], Loss: 0.0457\n",
            "Epoch [30/50], Loss: 0.1256\n",
            "Epoch [30/50], Loss: 0.0568\n",
            "Epoch [30/50], Loss: 0.0491\n",
            "Epoch [30/50], Loss: 0.1238\n",
            "Epoch [30/50], Loss: 0.2087\n",
            "Epoch [30/50], Loss: 0.0358\n",
            "Epoch [30/50], Loss: 0.0352\n",
            "Epoch [30/50], Loss: 0.0918\n",
            "Epoch [30/50], Loss: 0.0262\n",
            "Epoch [30/50], Loss: 0.0503\n",
            "Epoch [30/50], Loss: 0.0303\n",
            "Epoch [30/50], Loss: 0.0354\n",
            "Epoch [31/50], Loss: 0.0607\n",
            "Epoch [31/50], Loss: 0.0323\n",
            "Epoch [31/50], Loss: 0.0631\n",
            "Epoch [31/50], Loss: 0.0584\n",
            "Epoch [31/50], Loss: 0.0103\n",
            "Epoch [31/50], Loss: 0.1324\n",
            "Epoch [31/50], Loss: 0.1551\n",
            "Epoch [31/50], Loss: 0.0790\n",
            "Epoch [31/50], Loss: 0.0615\n",
            "Epoch [31/50], Loss: 0.0422\n",
            "Epoch [31/50], Loss: 0.0552\n",
            "Epoch [31/50], Loss: 0.1396\n",
            "Epoch [31/50], Loss: 0.0222\n",
            "Epoch [31/50], Loss: 0.0808\n",
            "Epoch [31/50], Loss: 0.0567\n",
            "Epoch [32/50], Loss: 0.0235\n",
            "Epoch [32/50], Loss: 0.1184\n",
            "Epoch [32/50], Loss: 0.0321\n",
            "Epoch [32/50], Loss: 0.1194\n",
            "Epoch [32/50], Loss: 0.0229\n",
            "Epoch [32/50], Loss: 0.0620\n",
            "Epoch [32/50], Loss: 0.0593\n",
            "Epoch [32/50], Loss: 0.1848\n",
            "Epoch [32/50], Loss: 0.0497\n",
            "Epoch [32/50], Loss: 0.0323\n",
            "Epoch [32/50], Loss: 0.0796\n",
            "Epoch [32/50], Loss: 0.0428\n",
            "Epoch [32/50], Loss: 0.1225\n",
            "Epoch [32/50], Loss: 0.0302\n",
            "Epoch [32/50], Loss: 0.0304\n",
            "Epoch [33/50], Loss: 0.0528\n",
            "Epoch [33/50], Loss: 0.0323\n",
            "Epoch [33/50], Loss: 0.1111\n",
            "Epoch [33/50], Loss: 0.1513\n",
            "Epoch [33/50], Loss: 0.0440\n",
            "Epoch [33/50], Loss: 0.1027\n",
            "Epoch [33/50], Loss: 0.0728\n",
            "Epoch [33/50], Loss: 0.0577\n",
            "Epoch [33/50], Loss: 0.0250\n",
            "Epoch [33/50], Loss: 0.0844\n",
            "Epoch [33/50], Loss: 0.0502\n",
            "Epoch [33/50], Loss: 0.0663\n",
            "Epoch [33/50], Loss: 0.1082\n",
            "Epoch [33/50], Loss: 0.0092\n",
            "Epoch [33/50], Loss: 0.0097\n",
            "Epoch [34/50], Loss: 0.1088\n",
            "Epoch [34/50], Loss: 0.0410\n",
            "Epoch [34/50], Loss: 0.0542\n",
            "Epoch [34/50], Loss: 0.0359\n",
            "Epoch [34/50], Loss: 0.1483\n",
            "Epoch [34/50], Loss: 0.0966\n",
            "Epoch [34/50], Loss: 0.0718\n",
            "Epoch [34/50], Loss: 0.0218\n",
            "Epoch [34/50], Loss: 0.1434\n",
            "Epoch [34/50], Loss: 0.0235\n",
            "Epoch [34/50], Loss: 0.0176\n",
            "Epoch [34/50], Loss: 0.0390\n",
            "Epoch [34/50], Loss: 0.0321\n",
            "Epoch [34/50], Loss: 0.1159\n",
            "Epoch [34/50], Loss: 0.0205\n",
            "Epoch [35/50], Loss: 0.1256\n",
            "Epoch [35/50], Loss: 0.0179\n",
            "Epoch [35/50], Loss: 0.0305\n",
            "Epoch [35/50], Loss: 0.0533\n",
            "Epoch [35/50], Loss: 0.2070\n",
            "Epoch [35/50], Loss: 0.0236\n",
            "Epoch [35/50], Loss: 0.0855\n",
            "Epoch [35/50], Loss: 0.0574\n",
            "Epoch [35/50], Loss: 0.0418\n",
            "Epoch [35/50], Loss: 0.1126\n",
            "Epoch [35/50], Loss: 0.0596\n",
            "Epoch [35/50], Loss: 0.0305\n",
            "Epoch [35/50], Loss: 0.0623\n",
            "Epoch [35/50], Loss: 0.0267\n",
            "Epoch [35/50], Loss: 0.0213\n",
            "Epoch [36/50], Loss: 0.0985\n",
            "Epoch [36/50], Loss: 0.0219\n",
            "Epoch [36/50], Loss: 0.1587\n",
            "Epoch [36/50], Loss: 0.0295\n",
            "Epoch [36/50], Loss: 0.0571\n",
            "Epoch [36/50], Loss: 0.0326\n",
            "Epoch [36/50], Loss: 0.0413\n",
            "Epoch [36/50], Loss: 0.0348\n",
            "Epoch [36/50], Loss: 0.0313\n",
            "Epoch [36/50], Loss: 0.0767\n",
            "Epoch [36/50], Loss: 0.0371\n",
            "Epoch [36/50], Loss: 0.0572\n",
            "Epoch [36/50], Loss: 0.0670\n",
            "Epoch [36/50], Loss: 0.0938\n",
            "Epoch [36/50], Loss: 0.4080\n",
            "Epoch [37/50], Loss: 0.1848\n",
            "Epoch [37/50], Loss: 0.0807\n",
            "Epoch [37/50], Loss: 0.0771\n",
            "Epoch [37/50], Loss: 0.0520\n",
            "Epoch [37/50], Loss: 0.0260\n",
            "Epoch [37/50], Loss: 0.0437\n",
            "Epoch [37/50], Loss: 0.0181\n",
            "Epoch [37/50], Loss: 0.0258\n",
            "Epoch [37/50], Loss: 0.0392\n",
            "Epoch [37/50], Loss: 0.0129\n",
            "Epoch [37/50], Loss: 0.0669\n",
            "Epoch [37/50], Loss: 0.0357\n",
            "Epoch [37/50], Loss: 0.0109\n",
            "Epoch [37/50], Loss: 0.1636\n",
            "Epoch [37/50], Loss: 0.3478\n",
            "Epoch [38/50], Loss: 0.1155\n",
            "Epoch [38/50], Loss: 0.0169\n",
            "Epoch [38/50], Loss: 0.0203\n",
            "Epoch [38/50], Loss: 0.0529\n",
            "Epoch [38/50], Loss: 0.0335\n",
            "Epoch [38/50], Loss: 0.0639\n",
            "Epoch [38/50], Loss: 0.0404\n",
            "Epoch [38/50], Loss: 0.0627\n",
            "Epoch [38/50], Loss: 0.1414\n",
            "Epoch [38/50], Loss: 0.1557\n",
            "Epoch [38/50], Loss: 0.0364\n",
            "Epoch [38/50], Loss: 0.0210\n",
            "Epoch [38/50], Loss: 0.0127\n",
            "Epoch [38/50], Loss: 0.0586\n",
            "Epoch [38/50], Loss: 0.3177\n",
            "Epoch [39/50], Loss: 0.0374\n",
            "Epoch [39/50], Loss: 0.0456\n",
            "Epoch [39/50], Loss: 0.1596\n",
            "Epoch [39/50], Loss: 0.0242\n",
            "Epoch [39/50], Loss: 0.0322\n",
            "Epoch [39/50], Loss: 0.1148\n",
            "Epoch [39/50], Loss: 0.0627\n",
            "Epoch [39/50], Loss: 0.0979\n",
            "Epoch [39/50], Loss: 0.0276\n",
            "Epoch [39/50], Loss: 0.0466\n",
            "Epoch [39/50], Loss: 0.0446\n",
            "Epoch [39/50], Loss: 0.0461\n",
            "Epoch [39/50], Loss: 0.0231\n",
            "Epoch [39/50], Loss: 0.1291\n",
            "Epoch [39/50], Loss: 0.0132\n",
            "Epoch [40/50], Loss: 0.1390\n",
            "Epoch [40/50], Loss: 0.0577\n",
            "Epoch [40/50], Loss: 0.0673\n",
            "Epoch [40/50], Loss: 0.0204\n",
            "Epoch [40/50], Loss: 0.0129\n",
            "Epoch [40/50], Loss: 0.0362\n",
            "Epoch [40/50], Loss: 0.1360\n",
            "Epoch [40/50], Loss: 0.0498\n",
            "Epoch [40/50], Loss: 0.1438\n",
            "Epoch [40/50], Loss: 0.0383\n",
            "Epoch [40/50], Loss: 0.0177\n",
            "Epoch [40/50], Loss: 0.0276\n",
            "Epoch [40/50], Loss: 0.0498\n",
            "Epoch [40/50], Loss: 0.0849\n",
            "Epoch [40/50], Loss: 0.0118\n",
            "Epoch [41/50], Loss: 0.0737\n",
            "Epoch [41/50], Loss: 0.0256\n",
            "Epoch [41/50], Loss: 0.0114\n",
            "Epoch [41/50], Loss: 0.0317\n",
            "Epoch [41/50], Loss: 0.0319\n",
            "Epoch [41/50], Loss: 0.0497\n",
            "Epoch [41/50], Loss: 0.1256\n",
            "Epoch [41/50], Loss: 0.0309\n",
            "Epoch [41/50], Loss: 0.0489\n",
            "Epoch [41/50], Loss: 0.1413\n",
            "Epoch [41/50], Loss: 0.0242\n",
            "Epoch [41/50], Loss: 0.0257\n",
            "Epoch [41/50], Loss: 0.1876\n",
            "Epoch [41/50], Loss: 0.0148\n",
            "Epoch [41/50], Loss: 0.2299\n",
            "Epoch [42/50], Loss: 0.0532\n",
            "Epoch [42/50], Loss: 0.0902\n",
            "Epoch [42/50], Loss: 0.0097\n",
            "Epoch [42/50], Loss: 0.0405\n",
            "Epoch [42/50], Loss: 0.0237\n",
            "Epoch [42/50], Loss: 0.0283\n",
            "Epoch [42/50], Loss: 0.0117\n",
            "Epoch [42/50], Loss: 0.0549\n",
            "Epoch [42/50], Loss: 0.0994\n",
            "Epoch [42/50], Loss: 0.0266\n",
            "Epoch [42/50], Loss: 0.0360\n",
            "Epoch [42/50], Loss: 0.2054\n",
            "Epoch [42/50], Loss: 0.1253\n",
            "Epoch [42/50], Loss: 0.0477\n",
            "Epoch [42/50], Loss: 0.0293\n",
            "Epoch [43/50], Loss: 0.0259\n",
            "Epoch [43/50], Loss: 0.0468\n",
            "Epoch [43/50], Loss: 0.0318\n",
            "Epoch [43/50], Loss: 0.0206\n",
            "Epoch [43/50], Loss: 0.1534\n",
            "Epoch [43/50], Loss: 0.0806\n",
            "Epoch [43/50], Loss: 0.0386\n",
            "Epoch [43/50], Loss: 0.0213\n",
            "Epoch [43/50], Loss: 0.0825\n",
            "Epoch [43/50], Loss: 0.0221\n",
            "Epoch [43/50], Loss: 0.0448\n",
            "Epoch [43/50], Loss: 0.0274\n",
            "Epoch [43/50], Loss: 0.1245\n",
            "Epoch [43/50], Loss: 0.0375\n",
            "Epoch [43/50], Loss: 0.4119\n",
            "Epoch [44/50], Loss: 0.0257\n",
            "Epoch [44/50], Loss: 0.0711\n",
            "Epoch [44/50], Loss: 0.2684\n",
            "Epoch [44/50], Loss: 0.1110\n",
            "Epoch [44/50], Loss: 0.0454\n",
            "Epoch [44/50], Loss: 0.0370\n",
            "Epoch [44/50], Loss: 0.0346\n",
            "Epoch [44/50], Loss: 0.0423\n",
            "Epoch [44/50], Loss: 0.0228\n",
            "Epoch [44/50], Loss: 0.0213\n",
            "Epoch [44/50], Loss: 0.0331\n",
            "Epoch [44/50], Loss: 0.0238\n",
            "Epoch [44/50], Loss: 0.0336\n",
            "Epoch [44/50], Loss: 0.0676\n",
            "Epoch [44/50], Loss: 0.0049\n",
            "Epoch [45/50], Loss: 0.0247\n",
            "Epoch [45/50], Loss: 0.0361\n",
            "Epoch [45/50], Loss: 0.0357\n",
            "Epoch [45/50], Loss: 0.0597\n",
            "Epoch [45/50], Loss: 0.1712\n",
            "Epoch [45/50], Loss: 0.0719\n",
            "Epoch [45/50], Loss: 0.1087\n",
            "Epoch [45/50], Loss: 0.0227\n",
            "Epoch [45/50], Loss: 0.0470\n",
            "Epoch [45/50], Loss: 0.0458\n",
            "Epoch [45/50], Loss: 0.0271\n",
            "Epoch [45/50], Loss: 0.0192\n",
            "Epoch [45/50], Loss: 0.0148\n",
            "Epoch [45/50], Loss: 0.1422\n",
            "Epoch [45/50], Loss: 0.0154\n",
            "Epoch [46/50], Loss: 0.0434\n",
            "Epoch [46/50], Loss: 0.0240\n",
            "Epoch [46/50], Loss: 0.0132\n",
            "Epoch [46/50], Loss: 0.0418\n",
            "Epoch [46/50], Loss: 0.0879\n",
            "Epoch [46/50], Loss: 0.0490\n",
            "Epoch [46/50], Loss: 0.0217\n",
            "Epoch [46/50], Loss: 0.0294\n",
            "Epoch [46/50], Loss: 0.0388\n",
            "Epoch [46/50], Loss: 0.0252\n",
            "Epoch [46/50], Loss: 0.0314\n",
            "Epoch [46/50], Loss: 0.2288\n",
            "Epoch [46/50], Loss: 0.0327\n",
            "Epoch [46/50], Loss: 0.1433\n",
            "Epoch [46/50], Loss: 0.0542\n",
            "Epoch [47/50], Loss: 0.1466\n",
            "Epoch [47/50], Loss: 0.0367\n",
            "Epoch [47/50], Loss: 0.0275\n",
            "Epoch [47/50], Loss: 0.0921\n",
            "Epoch [47/50], Loss: 0.0381\n",
            "Epoch [47/50], Loss: 0.0145\n",
            "Epoch [47/50], Loss: 0.0219\n",
            "Epoch [47/50], Loss: 0.0883\n",
            "Epoch [47/50], Loss: 0.0606\n",
            "Epoch [47/50], Loss: 0.0074\n",
            "Epoch [47/50], Loss: 0.0186\n",
            "Epoch [47/50], Loss: 0.0504\n",
            "Epoch [47/50], Loss: 0.1585\n",
            "Epoch [47/50], Loss: 0.0430\n",
            "Epoch [47/50], Loss: 0.0329\n",
            "Epoch [48/50], Loss: 0.1017\n",
            "Epoch [48/50], Loss: 0.1540\n",
            "Epoch [48/50], Loss: 0.1138\n",
            "Epoch [48/50], Loss: 0.0277\n",
            "Epoch [48/50], Loss: 0.0553\n",
            "Epoch [48/50], Loss: 0.0464\n",
            "Epoch [48/50], Loss: 0.0240\n",
            "Epoch [48/50], Loss: 0.0272\n",
            "Epoch [48/50], Loss: 0.0236\n",
            "Epoch [48/50], Loss: 0.0229\n",
            "Epoch [48/50], Loss: 0.0313\n",
            "Epoch [48/50], Loss: 0.0372\n",
            "Epoch [48/50], Loss: 0.1179\n",
            "Epoch [48/50], Loss: 0.0177\n",
            "Epoch [48/50], Loss: 0.0029\n",
            "Epoch [49/50], Loss: 0.1201\n",
            "Epoch [49/50], Loss: 0.0163\n",
            "Epoch [49/50], Loss: 0.0263\n",
            "Epoch [49/50], Loss: 0.0507\n",
            "Epoch [49/50], Loss: 0.0241\n",
            "Epoch [49/50], Loss: 0.0171\n",
            "Epoch [49/50], Loss: 0.2230\n",
            "Epoch [49/50], Loss: 0.0087\n",
            "Epoch [49/50], Loss: 0.1382\n",
            "Epoch [49/50], Loss: 0.0153\n",
            "Epoch [49/50], Loss: 0.0416\n",
            "Epoch [49/50], Loss: 0.0305\n",
            "Epoch [49/50], Loss: 0.0336\n",
            "Epoch [49/50], Loss: 0.0335\n",
            "Epoch [49/50], Loss: 0.0574\n",
            "Epoch [50/50], Loss: 0.1200\n",
            "Epoch [50/50], Loss: 0.0201\n",
            "Epoch [50/50], Loss: 0.0397\n",
            "Epoch [50/50], Loss: 0.0299\n",
            "Epoch [50/50], Loss: 0.0205\n",
            "Epoch [50/50], Loss: 0.0403\n",
            "Epoch [50/50], Loss: 0.1060\n",
            "Epoch [50/50], Loss: 0.0187\n",
            "Epoch [50/50], Loss: 0.0242\n",
            "Epoch [50/50], Loss: 0.0375\n",
            "Epoch [50/50], Loss: 0.0168\n",
            "Epoch [50/50], Loss: 0.0262\n",
            "Epoch [50/50], Loss: 0.1818\n",
            "Epoch [50/50], Loss: 0.1011\n",
            "Epoch [50/50], Loss: 0.0048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_t)\n",
        "    predicted = (outputs >= 0.5).float()\n",
        "    accuracy = (predicted.eq(y_test_t).sum().item() / y_test_t.size(0)) * 100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swVjiIfNf998",
        "outputId": "55a60712-32a6-4c46-e503-73dbbab61fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we see our accuarcy is increased from 85% to 95%."
      ],
      "metadata": {
        "id": "jjZW1BBsgtMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}