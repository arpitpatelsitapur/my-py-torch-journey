{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOesJLUqwk9QV90LaqvAQE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitpatelsitapur/my-py-torch-journey/blob/main/Fashion_MNIST_pytorch_ANN_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking in only 6000 rows of training and 1000 of testing**"
      ],
      "metadata": {
        "id": "Pn2zeoUyW0Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we load MNIST data from keras, there are other methods too.\n",
        "from keras import datasets\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "5fM5f2TLwPua"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S7JeFULzvuT1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "pO9snHVlwNQP",
        "outputId": "0e22f60c-db68-4a51-eab0-5a4c02a36581"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
              "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n",
              "2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n",
              "3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n",
              "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "\n",
              "   778  779  780  781  782  783  \n",
              "0    0    0    0    0    0    0  \n",
              "1    0    0    0    0    0    0  \n",
              "2    0    0    0    0    0    0  \n",
              "3    0    0    0    0    0    0  \n",
              "4    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f20b238-e9fb-411b-b975-30dcd8e883a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>119</td>\n",
              "      <td>114</td>\n",
              "      <td>130</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 784 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f20b238-e9fb-411b-b975-30dcd8e883a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4f20b238-e9fb-411b-b975-30dcd8e883a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4f20b238-e9fb-411b-b975-30dcd8e883a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d86702b2-e914-420e-b2df-f8ede2378b0b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d86702b2-e914-420e-b2df-f8ede2378b0b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d86702b2-e914-420e-b2df-f8ede2378b0b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_test.shape = \", X_test.shape)\n",
        "print(\"-\"*100)\n",
        "print(\"Keeping only 6000 in training and 1000 for testing.\")\n",
        "X_train=X_train.head(6000)\n",
        "X_test=X_test.head(1000)\n",
        "y_train=y_train.head(6000)\n",
        "y_test=y_test.head(1000)\n",
        "print(\"-\"*100)\n",
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_test.shape = \", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yfytyOqy6Wj",
        "outputId": "8a0fe645-48ab-4fa3-d1db-310480b13fec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape =  (60000, 784)\n",
            "X_test.shape =  (10000, 784)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Keeping only 6000 in training and 1000 for testing.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "X_train.shape =  (6000, 784)\n",
            "X_test.shape =  (1000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if running first time, u need to install torchinfo\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "ZaQ-UyMY1dms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ecfcb8-8fbf-41fb-8516-2d1bb43a9752"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZx3Q57Ry6S6",
        "outputId": "f64d5417-6cd9-4170-c321-fb72145982de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78043b524450>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Vtk9WnY6y6Py"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train_t.shape = {X_train_t.shape}, y_train_t.shape = {y_train_t.shape}\")\n",
        "print(f\"X_test_t.shape = {X_test_t.shape}, y_test_t.shape = {y_test_t.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWTqtYRBy6M7",
        "outputId": "9072bf65-fb2b-470b-c134-a1a71e8eb6e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_t.shape = torch.Size([6000, 784]), y_train_t.shape = torch.Size([6000, 1])\n",
            "X_test_t.shape = torch.Size([1000, 784]), y_test_t.shape = torch.Size([1000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define datset and dataloader\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=150,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=150,shuffle=True)\n",
        "\n",
        "for batch_X,batch_y in test_loader:\n",
        "  # print(batch_X)\n",
        "  # print(batch_y)\n",
        "  print(f\"batch_X.shape = {batch_X.shape}, batch_y.shape = {batch_y.shape}\")\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmEfxyBJy6Jy",
        "outputId": "8d110103-772e-426a-b758-16b5244c569a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([100, 784]), batch_y.shape = torch.Size([100, 1])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## **ANN model structure**\n",
        "# - input layer (784)\n",
        "# - 2 hidden layer (each 128)\n",
        "# - 1 output layer\n",
        "# - relu in hidden layers\n",
        "# - softmax in output layer\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))      # hidden layer1 with ReLU\n",
        "        x = torch.relu(self.fc2(x))      # hidden layer1 with ReLU\n",
        "        x = self.fc3(x)  # output layer (remove softmax, CrossEntropyLoss includes it)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tqUKLvK3y6GL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axwj84Sqy5-n",
        "outputId": "e6f3a9fb-485e-4008-d8b4-c174ac225f4f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "SimpleNN                                 --\n",
              "â”œâ”€Linear: 1-1                            100,480\n",
              "â”œâ”€Linear: 1-2                            16,512\n",
              "â”œâ”€Linear: 1-3                            1,290\n",
              "=================================================================\n",
              "Total params: 118,282\n",
              "Trainable params: 118,282\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrAps-oo3zL0",
        "outputId": "69f0050c-8bf7-4e71-a92a-c2baed65de3c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Avg Loss: 4.0186\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [2/50], Avg Loss: 0.6457\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [3/50], Avg Loss: 0.5392\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [4/50], Avg Loss: 0.4846\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [5/50], Avg Loss: 0.4359\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [6/50], Avg Loss: 0.4257\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [7/50], Avg Loss: 0.3871\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [8/50], Avg Loss: 0.3452\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [9/50], Avg Loss: 0.3299\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [10/50], Avg Loss: 0.3250\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [11/50], Avg Loss: 0.3232\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [12/50], Avg Loss: 0.2887\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [13/50], Avg Loss: 0.2787\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [14/50], Avg Loss: 0.2558\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [15/50], Avg Loss: 0.2558\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [16/50], Avg Loss: 0.2265\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [17/50], Avg Loss: 0.2362\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [18/50], Avg Loss: 0.2218\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [19/50], Avg Loss: 0.2289\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [20/50], Avg Loss: 0.2217\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [21/50], Avg Loss: 0.2328\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [22/50], Avg Loss: 0.2506\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [23/50], Avg Loss: 0.2284\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [24/50], Avg Loss: 0.2255\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [25/50], Avg Loss: 0.2383\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [26/50], Avg Loss: 0.2210\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [27/50], Avg Loss: 0.1710\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [28/50], Avg Loss: 0.1755\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [29/50], Avg Loss: 0.1679\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [30/50], Avg Loss: 0.1917\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [31/50], Avg Loss: 0.1518\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [32/50], Avg Loss: 0.1408\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [33/50], Avg Loss: 0.1671\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [34/50], Avg Loss: 0.1501\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [35/50], Avg Loss: 0.1404\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [36/50], Avg Loss: 0.1615\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [37/50], Avg Loss: 0.1495\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [38/50], Avg Loss: 0.1367\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [39/50], Avg Loss: 0.1227\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [40/50], Avg Loss: 0.1397\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [41/50], Avg Loss: 0.1371\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [42/50], Avg Loss: 0.1301\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [43/50], Avg Loss: 0.1258\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [44/50], Avg Loss: 0.1219\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [45/50], Avg Loss: 0.1068\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [46/50], Avg Loss: 0.1208\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [47/50], Avg Loss: 0.1750\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [48/50], Avg Loss: 0.1408\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [49/50], Avg Loss: 0.1477\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [50/50], Avg Loss: 0.1810\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "G4P5-5SiFqC9",
        "outputId": "69ad85cd-37bb-4324-b5c5-886b6b5083e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNN(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 1000 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vYMZxVf3y-0",
        "outputId": "8fca606f-ebf4-4663-99c2-1e33e7cd6b84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 1000 test images: 82.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIeyQFNgEIGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applying this ANN in complete data**"
      ],
      "metadata": {
        "id": "QXd_ShBjXCDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(16)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=32,shuffle=False)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rXCN6kfEIB2",
        "outputId": "9d3a7b6e-0c47-4385-8277-74c0d0e96825"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "â”œâ”€Sequential: 1-1                        --\n",
            "â”‚    â””â”€Linear: 2-1                       100,480\n",
            "â”‚    â””â”€ReLU: 2-2                         --\n",
            "â”‚    â””â”€Linear: 2-3                       16,512\n",
            "â”‚    â””â”€ReLU: 2-4                         --\n",
            "â”‚    â””â”€Linear: 2-5                       1,290\n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/50], Loss: 0.6750\n",
            "=================================================================\n",
            "Epoch [2/50], Loss: 0.4707\n",
            "=================================================================\n",
            "Epoch [3/50], Loss: 0.4279\n",
            "=================================================================\n",
            "Epoch [4/50], Loss: 0.4060\n",
            "=================================================================\n",
            "Epoch [5/50], Loss: 0.3921\n",
            "=================================================================\n",
            "Epoch [6/50], Loss: 0.3831\n",
            "=================================================================\n",
            "Epoch [7/50], Loss: 0.3714\n",
            "=================================================================\n",
            "Epoch [8/50], Loss: 0.3606\n",
            "=================================================================\n",
            "Epoch [9/50], Loss: 0.3515\n",
            "=================================================================\n",
            "Epoch [10/50], Loss: 0.3521\n",
            "=================================================================\n",
            "Epoch [11/50], Loss: 0.3418\n",
            "=================================================================\n",
            "Epoch [12/50], Loss: 0.3400\n",
            "=================================================================\n",
            "Epoch [13/50], Loss: 0.3368\n",
            "=================================================================\n",
            "Epoch [14/50], Loss: 0.3325\n",
            "=================================================================\n",
            "Epoch [15/50], Loss: 0.3288\n",
            "=================================================================\n",
            "Epoch [16/50], Loss: 0.3250\n",
            "=================================================================\n",
            "Epoch [17/50], Loss: 0.3327\n",
            "=================================================================\n",
            "Epoch [18/50], Loss: 0.3198\n",
            "=================================================================\n",
            "Epoch [19/50], Loss: 0.3200\n",
            "=================================================================\n",
            "Epoch [20/50], Loss: 0.3241\n",
            "=================================================================\n",
            "Epoch [21/50], Loss: 0.3144\n",
            "=================================================================\n",
            "Epoch [22/50], Loss: 0.3184\n",
            "=================================================================\n",
            "Epoch [23/50], Loss: 0.3071\n",
            "=================================================================\n",
            "Epoch [24/50], Loss: 0.3145\n",
            "=================================================================\n",
            "Epoch [25/50], Loss: 0.3096\n",
            "=================================================================\n",
            "Epoch [26/50], Loss: 0.3053\n",
            "=================================================================\n",
            "Epoch [27/50], Loss: 0.3067\n",
            "=================================================================\n",
            "Epoch [28/50], Loss: 0.3105\n",
            "=================================================================\n",
            "Epoch [29/50], Loss: 0.3035\n",
            "=================================================================\n",
            "Epoch [30/50], Loss: 0.3115\n",
            "=================================================================\n",
            "Epoch [31/50], Loss: 0.3061\n",
            "=================================================================\n",
            "Epoch [32/50], Loss: 0.3009\n",
            "=================================================================\n",
            "Epoch [33/50], Loss: 0.3106\n",
            "=================================================================\n",
            "Epoch [34/50], Loss: 0.3045\n",
            "=================================================================\n",
            "Epoch [35/50], Loss: 0.2954\n",
            "=================================================================\n",
            "Epoch [36/50], Loss: 0.3001\n",
            "=================================================================\n",
            "Epoch [37/50], Loss: 0.2917\n",
            "=================================================================\n",
            "Epoch [38/50], Loss: 0.2924\n",
            "=================================================================\n",
            "Epoch [39/50], Loss: 0.2927\n",
            "=================================================================\n",
            "Epoch [40/50], Loss: 0.3030\n",
            "=================================================================\n",
            "Epoch [41/50], Loss: 0.2923\n",
            "=================================================================\n",
            "Epoch [42/50], Loss: 0.2923\n",
            "=================================================================\n",
            "Epoch [43/50], Loss: 0.2930\n",
            "=================================================================\n",
            "Epoch [44/50], Loss: 0.2883\n",
            "=================================================================\n",
            "Epoch [45/50], Loss: 0.2921\n",
            "=================================================================\n",
            "Epoch [46/50], Loss: 0.2933\n",
            "=================================================================\n",
            "Epoch [47/50], Loss: 0.2818\n",
            "=================================================================\n",
            "Epoch [48/50], Loss: 0.2850\n",
            "=================================================================\n",
            "Epoch [49/50], Loss: 0.2909\n",
            "=================================================================\n",
            "Epoch [50/50], Loss: 0.2913\n",
            "=================================================================\n",
            "Accuracy of the network on the test images: 85.74 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jm1JyInVEH8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using GPU to Speedup Process**\n",
        "1. Check for gpu device availability.\n",
        "2. move model to gpu.\n",
        "3. use `pin_memory=True` parameter for faster data loading.\n",
        "4. move batches to gpu in both training and testing phase.\n",
        "5. use larger size batches."
      ],
      "metadata": {
        "id": "wNoi_b1WXNxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=256,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=256,shuffle=False,pin_memory=True)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 100\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "# move model to GPU\n",
        "model = model.to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "# checking accuracy in training data\n",
        "# checking if our model is overfitted\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the training images: {acc} %')\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the test images: {acc} %')"
      ],
      "metadata": {
        "id": "rrU304VjXNg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c6328c-094a-4cf3-8eb5-e714d0c37356"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "â”œâ”€Sequential: 1-1                        --\n",
            "â”‚    â””â”€Linear: 2-1                       100,480\n",
            "â”‚    â””â”€ReLU: 2-2                         --\n",
            "â”‚    â””â”€Linear: 2-3                       16,512\n",
            "â”‚    â””â”€ReLU: 2-4                         --\n",
            "â”‚    â””â”€Linear: 2-5                       1,290\n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/100], Avg Loss: 0.9130\n",
            "=================================================================\n",
            "Epoch [2/100], Avg Loss: 0.4426\n",
            "=================================================================\n",
            "Epoch [3/100], Avg Loss: 0.3908\n",
            "=================================================================\n",
            "Epoch [4/100], Avg Loss: 0.3624\n",
            "=================================================================\n",
            "Epoch [5/100], Avg Loss: 0.3423\n",
            "=================================================================\n",
            "Epoch [6/100], Avg Loss: 0.3339\n",
            "=================================================================\n",
            "Epoch [7/100], Avg Loss: 0.3168\n",
            "=================================================================\n",
            "Epoch [8/100], Avg Loss: 0.3188\n",
            "=================================================================\n",
            "Epoch [9/100], Avg Loss: 0.3109\n",
            "=================================================================\n",
            "Epoch [10/100], Avg Loss: 0.2953\n",
            "=================================================================\n",
            "Epoch [11/100], Avg Loss: 0.2920\n",
            "=================================================================\n",
            "Epoch [12/100], Avg Loss: 0.2829\n",
            "=================================================================\n",
            "Epoch [13/100], Avg Loss: 0.2831\n",
            "=================================================================\n",
            "Epoch [14/100], Avg Loss: 0.2787\n",
            "=================================================================\n",
            "Epoch [15/100], Avg Loss: 0.2679\n",
            "=================================================================\n",
            "Epoch [16/100], Avg Loss: 0.2728\n",
            "=================================================================\n",
            "Epoch [17/100], Avg Loss: 0.2606\n",
            "=================================================================\n",
            "Epoch [18/100], Avg Loss: 0.2606\n",
            "=================================================================\n",
            "Epoch [19/100], Avg Loss: 0.2550\n",
            "=================================================================\n",
            "Epoch [20/100], Avg Loss: 0.2506\n",
            "=================================================================\n",
            "Epoch [21/100], Avg Loss: 0.2456\n",
            "=================================================================\n",
            "Epoch [22/100], Avg Loss: 0.2458\n",
            "=================================================================\n",
            "Epoch [23/100], Avg Loss: 0.2370\n",
            "=================================================================\n",
            "Epoch [24/100], Avg Loss: 0.2340\n",
            "=================================================================\n",
            "Epoch [25/100], Avg Loss: 0.2336\n",
            "=================================================================\n",
            "Epoch [26/100], Avg Loss: 0.2346\n",
            "=================================================================\n",
            "Epoch [27/100], Avg Loss: 0.2339\n",
            "=================================================================\n",
            "Epoch [28/100], Avg Loss: 0.2330\n",
            "=================================================================\n",
            "Epoch [29/100], Avg Loss: 0.2259\n",
            "=================================================================\n",
            "Epoch [30/100], Avg Loss: 0.2184\n",
            "=================================================================\n",
            "Epoch [31/100], Avg Loss: 0.2175\n",
            "=================================================================\n",
            "Epoch [32/100], Avg Loss: 0.2210\n",
            "=================================================================\n",
            "Epoch [33/100], Avg Loss: 0.2260\n",
            "=================================================================\n",
            "Epoch [34/100], Avg Loss: 0.2204\n",
            "=================================================================\n",
            "Epoch [35/100], Avg Loss: 0.2212\n",
            "=================================================================\n",
            "Epoch [36/100], Avg Loss: 0.2156\n",
            "=================================================================\n",
            "Epoch [37/100], Avg Loss: 0.2119\n",
            "=================================================================\n",
            "Epoch [38/100], Avg Loss: 0.2088\n",
            "=================================================================\n",
            "Epoch [39/100], Avg Loss: 0.2024\n",
            "=================================================================\n",
            "Epoch [40/100], Avg Loss: 0.2047\n",
            "=================================================================\n",
            "Epoch [41/100], Avg Loss: 0.2048\n",
            "=================================================================\n",
            "Epoch [42/100], Avg Loss: 0.1945\n",
            "=================================================================\n",
            "Epoch [43/100], Avg Loss: 0.1993\n",
            "=================================================================\n",
            "Epoch [44/100], Avg Loss: 0.1937\n",
            "=================================================================\n",
            "Epoch [45/100], Avg Loss: 0.1919\n",
            "=================================================================\n",
            "Epoch [46/100], Avg Loss: 0.1972\n",
            "=================================================================\n",
            "Epoch [47/100], Avg Loss: 0.1870\n",
            "=================================================================\n",
            "Epoch [48/100], Avg Loss: 0.1969\n",
            "=================================================================\n",
            "Epoch [49/100], Avg Loss: 0.1907\n",
            "=================================================================\n",
            "Epoch [50/100], Avg Loss: 0.1796\n",
            "=================================================================\n",
            "Epoch [51/100], Avg Loss: 0.1810\n",
            "=================================================================\n",
            "Epoch [52/100], Avg Loss: 0.1879\n",
            "=================================================================\n",
            "Epoch [53/100], Avg Loss: 0.1885\n",
            "=================================================================\n",
            "Epoch [54/100], Avg Loss: 0.1852\n",
            "=================================================================\n",
            "Epoch [55/100], Avg Loss: 0.1762\n",
            "=================================================================\n",
            "Epoch [56/100], Avg Loss: 0.1812\n",
            "=================================================================\n",
            "Epoch [57/100], Avg Loss: 0.1735\n",
            "=================================================================\n",
            "Epoch [58/100], Avg Loss: 0.1729\n",
            "=================================================================\n",
            "Epoch [59/100], Avg Loss: 0.1715\n",
            "=================================================================\n",
            "Epoch [60/100], Avg Loss: 0.1741\n",
            "=================================================================\n",
            "Epoch [61/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [62/100], Avg Loss: 0.1778\n",
            "=================================================================\n",
            "Epoch [63/100], Avg Loss: 0.1700\n",
            "=================================================================\n",
            "Epoch [64/100], Avg Loss: 0.1604\n",
            "=================================================================\n",
            "Epoch [65/100], Avg Loss: 0.1690\n",
            "=================================================================\n",
            "Epoch [66/100], Avg Loss: 0.1771\n",
            "=================================================================\n",
            "Epoch [67/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [68/100], Avg Loss: 0.1731\n",
            "=================================================================\n",
            "Epoch [69/100], Avg Loss: 0.1613\n",
            "=================================================================\n",
            "Epoch [70/100], Avg Loss: 0.1606\n",
            "=================================================================\n",
            "Epoch [71/100], Avg Loss: 0.1675\n",
            "=================================================================\n",
            "Epoch [72/100], Avg Loss: 0.1583\n",
            "=================================================================\n",
            "Epoch [73/100], Avg Loss: 0.1507\n",
            "=================================================================\n",
            "Epoch [74/100], Avg Loss: 0.1562\n",
            "=================================================================\n",
            "Epoch [75/100], Avg Loss: 0.1658\n",
            "=================================================================\n",
            "Epoch [76/100], Avg Loss: 0.1575\n",
            "=================================================================\n",
            "Epoch [77/100], Avg Loss: 0.1554\n",
            "=================================================================\n",
            "Epoch [78/100], Avg Loss: 0.1508\n",
            "=================================================================\n",
            "Epoch [79/100], Avg Loss: 0.1509\n",
            "=================================================================\n",
            "Epoch [80/100], Avg Loss: 0.1538\n",
            "=================================================================\n",
            "Epoch [81/100], Avg Loss: 0.1479\n",
            "=================================================================\n",
            "Epoch [82/100], Avg Loss: 0.1567\n",
            "=================================================================\n",
            "Epoch [83/100], Avg Loss: 0.1545\n",
            "=================================================================\n",
            "Epoch [84/100], Avg Loss: 0.1617\n",
            "=================================================================\n",
            "Epoch [85/100], Avg Loss: 0.1491\n",
            "=================================================================\n",
            "Epoch [86/100], Avg Loss: 0.1380\n",
            "=================================================================\n",
            "Epoch [87/100], Avg Loss: 0.1453\n",
            "=================================================================\n",
            "Epoch [88/100], Avg Loss: 0.1424\n",
            "=================================================================\n",
            "Epoch [89/100], Avg Loss: 0.1557\n",
            "=================================================================\n",
            "Epoch [90/100], Avg Loss: 0.1356\n",
            "=================================================================\n",
            "Epoch [91/100], Avg Loss: 0.1396\n",
            "=================================================================\n",
            "Epoch [92/100], Avg Loss: 0.1492\n",
            "=================================================================\n",
            "Epoch [93/100], Avg Loss: 0.1480\n",
            "=================================================================\n",
            "Epoch [94/100], Avg Loss: 0.1498\n",
            "=================================================================\n",
            "Epoch [95/100], Avg Loss: 0.1364\n",
            "=================================================================\n",
            "Epoch [96/100], Avg Loss: 0.1431\n",
            "=================================================================\n",
            "Epoch [97/100], Avg Loss: 0.1373\n",
            "=================================================================\n",
            "Epoch [98/100], Avg Loss: 0.1315\n",
            "=================================================================\n",
            "Epoch [99/100], Avg Loss: 0.1224\n",
            "=================================================================\n",
            "Epoch [100/100], Avg Loss: 0.1489\n",
            "=================================================================\n",
            "Accuracy of the network on the training images: 95.01 %\n",
            "Accuracy of the network on the test images: 87.85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizing model performance**\n",
        "- Dropout using `p=0.3`\n",
        "- Batch Normalization using `BatchNorm1D`\n",
        "- Early stopping\n",
        "- Regularization using `weight_decay`\n",
        "- Adding more data\n",
        "- Reducing Network complexity\n",
        "- Data augmentation"
      ],
      "metadata": {
        "id": "9wyo218zCczJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=256,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=256,shuffle=False,pin_memory=True)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 100\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "# move model to GPU\n",
        "model = model.to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "# checking accuracy in training data\n",
        "# checking if our model is overfitted\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the training images: {acc} %')\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing accuracy\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the testing images: {acc} %')"
      ],
      "metadata": {
        "id": "PbXzdrV3C_Od",
        "outputId": "743456a3-48ed-41b8-ba0b-959abda63b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "â”œâ”€Sequential: 1-1                        --\n",
            "â”‚    â””â”€Linear: 2-1                       100,480\n",
            "â”‚    â””â”€BatchNorm1d: 2-2                  256\n",
            "â”‚    â””â”€ReLU: 2-3                         --\n",
            "â”‚    â””â”€Dropout: 2-4                      --\n",
            "â”‚    â””â”€Linear: 2-5                       16,512\n",
            "â”‚    â””â”€BatchNorm1d: 2-6                  256\n",
            "â”‚    â””â”€ReLU: 2-7                         --\n",
            "â”‚    â””â”€Dropout: 2-8                      --\n",
            "â”‚    â””â”€Linear: 2-9                       1,290\n",
            "=================================================================\n",
            "Total params: 118,794\n",
            "Trainable params: 118,794\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/100], Avg Loss: 0.6251\n",
            "=================================================================\n",
            "Epoch [2/100], Avg Loss: 0.4244\n",
            "=================================================================\n",
            "Epoch [3/100], Avg Loss: 0.3863\n",
            "=================================================================\n",
            "Epoch [4/100], Avg Loss: 0.3614\n",
            "=================================================================\n",
            "Epoch [5/100], Avg Loss: 0.3419\n",
            "=================================================================\n",
            "Epoch [6/100], Avg Loss: 0.3316\n",
            "=================================================================\n",
            "Epoch [7/100], Avg Loss: 0.3208\n",
            "=================================================================\n",
            "Epoch [8/100], Avg Loss: 0.3135\n",
            "=================================================================\n",
            "Epoch [9/100], Avg Loss: 0.3056\n",
            "=================================================================\n",
            "Epoch [10/100], Avg Loss: 0.2982\n",
            "=================================================================\n",
            "Epoch [11/100], Avg Loss: 0.2918\n",
            "=================================================================\n",
            "Epoch [12/100], Avg Loss: 0.2797\n",
            "=================================================================\n",
            "Epoch [13/100], Avg Loss: 0.2777\n",
            "=================================================================\n",
            "Epoch [14/100], Avg Loss: 0.2730\n",
            "=================================================================\n",
            "Epoch [15/100], Avg Loss: 0.2688\n",
            "=================================================================\n",
            "Epoch [16/100], Avg Loss: 0.2624\n",
            "=================================================================\n",
            "Epoch [17/100], Avg Loss: 0.2616\n",
            "=================================================================\n",
            "Epoch [18/100], Avg Loss: 0.2577\n",
            "=================================================================\n",
            "Epoch [19/100], Avg Loss: 0.2526\n",
            "=================================================================\n",
            "Epoch [20/100], Avg Loss: 0.2454\n",
            "=================================================================\n",
            "Epoch [21/100], Avg Loss: 0.2488\n",
            "=================================================================\n",
            "Epoch [22/100], Avg Loss: 0.2449\n",
            "=================================================================\n",
            "Epoch [23/100], Avg Loss: 0.2379\n",
            "=================================================================\n",
            "Epoch [24/100], Avg Loss: 0.2365\n",
            "=================================================================\n",
            "Epoch [25/100], Avg Loss: 0.2336\n",
            "=================================================================\n",
            "Epoch [26/100], Avg Loss: 0.2320\n",
            "=================================================================\n",
            "Epoch [27/100], Avg Loss: 0.2322\n",
            "=================================================================\n",
            "Epoch [28/100], Avg Loss: 0.2277\n",
            "=================================================================\n",
            "Epoch [29/100], Avg Loss: 0.2242\n",
            "=================================================================\n",
            "Epoch [30/100], Avg Loss: 0.2230\n",
            "=================================================================\n",
            "Epoch [31/100], Avg Loss: 0.2174\n",
            "=================================================================\n",
            "Epoch [32/100], Avg Loss: 0.2172\n",
            "=================================================================\n",
            "Epoch [33/100], Avg Loss: 0.2186\n",
            "=================================================================\n",
            "Epoch [34/100], Avg Loss: 0.2183\n",
            "=================================================================\n",
            "Epoch [35/100], Avg Loss: 0.2098\n",
            "=================================================================\n",
            "Epoch [36/100], Avg Loss: 0.2136\n",
            "=================================================================\n",
            "Epoch [37/100], Avg Loss: 0.2127\n",
            "=================================================================\n",
            "Epoch [38/100], Avg Loss: 0.2078\n",
            "=================================================================\n",
            "Epoch [39/100], Avg Loss: 0.2101\n",
            "=================================================================\n",
            "Epoch [40/100], Avg Loss: 0.2067\n",
            "=================================================================\n",
            "Epoch [41/100], Avg Loss: 0.2013\n",
            "=================================================================\n",
            "Epoch [42/100], Avg Loss: 0.2036\n",
            "=================================================================\n",
            "Epoch [43/100], Avg Loss: 0.2032\n",
            "=================================================================\n",
            "Epoch [44/100], Avg Loss: 0.2013\n",
            "=================================================================\n",
            "Epoch [45/100], Avg Loss: 0.2012\n",
            "=================================================================\n",
            "Epoch [46/100], Avg Loss: 0.1999\n",
            "=================================================================\n",
            "Epoch [47/100], Avg Loss: 0.1981\n",
            "=================================================================\n",
            "Epoch [48/100], Avg Loss: 0.1958\n",
            "=================================================================\n",
            "Epoch [49/100], Avg Loss: 0.1945\n",
            "=================================================================\n",
            "Epoch [50/100], Avg Loss: 0.1937\n",
            "=================================================================\n",
            "Epoch [51/100], Avg Loss: 0.1934\n",
            "=================================================================\n",
            "Epoch [52/100], Avg Loss: 0.1942\n",
            "=================================================================\n",
            "Epoch [53/100], Avg Loss: 0.1898\n",
            "=================================================================\n",
            "Epoch [54/100], Avg Loss: 0.1872\n",
            "=================================================================\n",
            "Epoch [55/100], Avg Loss: 0.1894\n",
            "=================================================================\n",
            "Epoch [56/100], Avg Loss: 0.1867\n",
            "=================================================================\n",
            "Epoch [57/100], Avg Loss: 0.1899\n",
            "=================================================================\n",
            "Epoch [58/100], Avg Loss: 0.1864\n",
            "=================================================================\n",
            "Epoch [59/100], Avg Loss: 0.1831\n",
            "=================================================================\n",
            "Epoch [60/100], Avg Loss: 0.1853\n",
            "=================================================================\n",
            "Epoch [61/100], Avg Loss: 0.1830\n",
            "=================================================================\n",
            "Epoch [62/100], Avg Loss: 0.1854\n",
            "=================================================================\n",
            "Epoch [63/100], Avg Loss: 0.1810\n",
            "=================================================================\n",
            "Epoch [64/100], Avg Loss: 0.1811\n",
            "=================================================================\n",
            "Epoch [65/100], Avg Loss: 0.1824\n",
            "=================================================================\n",
            "Epoch [66/100], Avg Loss: 0.1803\n",
            "=================================================================\n",
            "Epoch [67/100], Avg Loss: 0.1767\n",
            "=================================================================\n",
            "Epoch [68/100], Avg Loss: 0.1774\n",
            "=================================================================\n",
            "Epoch [69/100], Avg Loss: 0.1764\n",
            "=================================================================\n",
            "Epoch [70/100], Avg Loss: 0.1755\n",
            "=================================================================\n",
            "Epoch [71/100], Avg Loss: 0.1763\n",
            "=================================================================\n",
            "Epoch [72/100], Avg Loss: 0.1745\n",
            "=================================================================\n",
            "Epoch [73/100], Avg Loss: 0.1761\n",
            "=================================================================\n",
            "Epoch [74/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [75/100], Avg Loss: 0.1759\n",
            "=================================================================\n",
            "Epoch [76/100], Avg Loss: 0.1752\n",
            "=================================================================\n",
            "Epoch [77/100], Avg Loss: 0.1733\n",
            "=================================================================\n",
            "Epoch [78/100], Avg Loss: 0.1719\n",
            "=================================================================\n",
            "Epoch [79/100], Avg Loss: 0.1705\n",
            "=================================================================\n",
            "Epoch [80/100], Avg Loss: 0.1735\n",
            "=================================================================\n",
            "Epoch [81/100], Avg Loss: 0.1688\n",
            "=================================================================\n",
            "Epoch [82/100], Avg Loss: 0.1680\n",
            "=================================================================\n",
            "Epoch [83/100], Avg Loss: 0.1687\n",
            "=================================================================\n",
            "Epoch [84/100], Avg Loss: 0.1678\n",
            "=================================================================\n",
            "Epoch [85/100], Avg Loss: 0.1665\n",
            "=================================================================\n",
            "Epoch [86/100], Avg Loss: 0.1680\n",
            "=================================================================\n",
            "Epoch [87/100], Avg Loss: 0.1652\n",
            "=================================================================\n",
            "Epoch [88/100], Avg Loss: 0.1698\n",
            "=================================================================\n",
            "Epoch [89/100], Avg Loss: 0.1654\n",
            "=================================================================\n",
            "Epoch [90/100], Avg Loss: 0.1639\n",
            "=================================================================\n",
            "Epoch [91/100], Avg Loss: 0.1687\n",
            "=================================================================\n",
            "Epoch [92/100], Avg Loss: 0.1656\n",
            "=================================================================\n",
            "Epoch [93/100], Avg Loss: 0.1652\n",
            "=================================================================\n",
            "Epoch [94/100], Avg Loss: 0.1625\n",
            "=================================================================\n",
            "Epoch [95/100], Avg Loss: 0.1656\n",
            "=================================================================\n",
            "Epoch [96/100], Avg Loss: 0.1638\n",
            "=================================================================\n",
            "Epoch [97/100], Avg Loss: 0.1650\n",
            "=================================================================\n",
            "Epoch [98/100], Avg Loss: 0.1658\n",
            "=================================================================\n",
            "Epoch [99/100], Avg Loss: 0.1603\n",
            "=================================================================\n",
            "Epoch [100/100], Avg Loss: 0.1614\n",
            "=================================================================\n",
            "Accuracy of the network on the training images: 94.20166666666667 %\n",
            "Accuracy of the network on the testing images: 89.26 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bkxf3fZCGVCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xxUXj77GU9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_0TWliaGU3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}