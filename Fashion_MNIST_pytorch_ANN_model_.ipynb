{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPufTAZXYdXD9hUCjTuR1jV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitpatelsitapur/my-py-torch-journey/blob/main/Fashion_MNIST_pytorch_ANN_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking in only 6000 rows of training and 1000 of testing**"
      ],
      "metadata": {
        "id": "Pn2zeoUyW0Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we load MNIST data from keras, there are other methods too.\n",
        "from keras import datasets\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "5fM5f2TLwPua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8472a4-37ab-4bc2-bf92-4844c720ac63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S7JeFULzvuT1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "pO9snHVlwNQP",
        "outputId": "0ef46703-2f39-4d87-e694-efbe2a11ca66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
              "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n",
              "2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n",
              "3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n",
              "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
              "\n",
              "   778  779  780  781  782  783  \n",
              "0    0    0    0    0    0    0  \n",
              "1    0    0    0    0    0    0  \n",
              "2    0    0    0    0    0    0  \n",
              "3    0    0    0    0    0    0  \n",
              "4    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-795cd119-9277-4738-913a-6a08d0168dce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>119</td>\n",
              "      <td>114</td>\n",
              "      <td>130</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-795cd119-9277-4738-913a-6a08d0168dce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-795cd119-9277-4738-913a-6a08d0168dce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-795cd119-9277-4738-913a-6a08d0168dce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-43aadb6b-e20b-4adc-a0df-e38ef34adddd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43aadb6b-e20b-4adc-a0df-e38ef34adddd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-43aadb6b-e20b-4adc-a0df-e38ef34adddd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_test.shape = \", X_test.shape)\n",
        "print(\"-\"*100)\n",
        "print(\"Keeping only 6000 in training and 1000 for testing.\")\n",
        "X_train=X_train.head(6000)\n",
        "X_test=X_test.head(1000)\n",
        "y_train=y_train.head(6000)\n",
        "y_test=y_test.head(1000)\n",
        "print(\"-\"*100)\n",
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_test.shape = \", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yfytyOqy6Wj",
        "outputId": "f30df904-8f7f-4d5e-fd9a-ffc7fca99c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape =  (60000, 784)\n",
            "X_test.shape =  (10000, 784)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Keeping only 6000 in training and 1000 for testing.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "X_train.shape =  (6000, 784)\n",
            "X_test.shape =  (1000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if running first time, u need to install torchinfo\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "ZaQ-UyMY1dms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4cb25e-6344-44a8-d115-5716fd8182ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZx3Q57Ry6S6",
        "outputId": "f787667b-4893-428f-e7bc-00fffffe06f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a723a2fef30>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Vtk9WnY6y6Py"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train_t.shape = {X_train_t.shape}, y_train_t.shape = {y_train_t.shape}\")\n",
        "print(f\"X_test_t.shape = {X_test_t.shape}, y_test_t.shape = {y_test_t.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWTqtYRBy6M7",
        "outputId": "164986c7-433c-431f-ddcd-87f6fdf3af09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_t.shape = torch.Size([6000, 784]), y_train_t.shape = torch.Size([6000, 1])\n",
            "X_test_t.shape = torch.Size([1000, 784]), y_test_t.shape = torch.Size([1000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define datset and dataloader\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=150,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=150,shuffle=True)\n",
        "\n",
        "for batch_X,batch_y in test_loader:\n",
        "  # print(batch_X)\n",
        "  # print(batch_y)\n",
        "  print(f\"batch_X.shape = {batch_X.shape}, batch_y.shape = {batch_y.shape}\")\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmEfxyBJy6Jy",
        "outputId": "74c53ad3-4ee2-4944-c78a-de01dd35f2ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([150, 784]), batch_y.shape = torch.Size([150, 1])\n",
            "--------------------------------------------------\n",
            "batch_X.shape = torch.Size([100, 784]), batch_y.shape = torch.Size([100, 1])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## **ANN model structure**\n",
        "# - input layer (784)\n",
        "# - 2 hidden layer (each 128)\n",
        "# - 1 output layer\n",
        "# - relu in hidden layers\n",
        "# - softmax in output layer\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))      # hidden layer1 with ReLU\n",
        "        x = torch.relu(self.fc2(x))      # hidden layer1 with ReLU\n",
        "        x = self.fc3(x)  # output layer (remove softmax, CrossEntropyLoss includes it)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tqUKLvK3y6GL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axwj84Sqy5-n",
        "outputId": "868b84c8-5bdb-4b54-eabf-0e18962649ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "SimpleNN                                 --\n",
              "├─Linear: 1-1                            100,480\n",
              "├─Linear: 1-2                            16,512\n",
              "├─Linear: 1-3                            1,290\n",
              "=================================================================\n",
              "Total params: 118,282\n",
              "Trainable params: 118,282\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrAps-oo3zL0",
        "outputId": "d4a9c994-f6dc-44ae-8a88-cae893a9bd93"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Avg Loss: 4.0186\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [2/50], Avg Loss: 0.6457\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [3/50], Avg Loss: 0.5392\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [4/50], Avg Loss: 0.4846\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [5/50], Avg Loss: 0.4359\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [6/50], Avg Loss: 0.4257\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [7/50], Avg Loss: 0.3871\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [8/50], Avg Loss: 0.3452\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [9/50], Avg Loss: 0.3299\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [10/50], Avg Loss: 0.3250\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [11/50], Avg Loss: 0.3232\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [12/50], Avg Loss: 0.2887\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [13/50], Avg Loss: 0.2787\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [14/50], Avg Loss: 0.2558\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [15/50], Avg Loss: 0.2558\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [16/50], Avg Loss: 0.2265\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [17/50], Avg Loss: 0.2362\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [18/50], Avg Loss: 0.2218\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [19/50], Avg Loss: 0.2292\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [20/50], Avg Loss: 0.2194\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [21/50], Avg Loss: 0.2546\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [22/50], Avg Loss: 0.2349\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [23/50], Avg Loss: 0.2297\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [24/50], Avg Loss: 0.2225\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [25/50], Avg Loss: 0.1940\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [26/50], Avg Loss: 0.1947\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [27/50], Avg Loss: 0.1652\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [28/50], Avg Loss: 0.1611\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [29/50], Avg Loss: 0.1667\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [30/50], Avg Loss: 0.2000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [31/50], Avg Loss: 0.1492\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [32/50], Avg Loss: 0.1637\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [33/50], Avg Loss: 0.1460\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [34/50], Avg Loss: 0.1638\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [35/50], Avg Loss: 0.1585\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [36/50], Avg Loss: 0.1540\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [37/50], Avg Loss: 0.1310\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [38/50], Avg Loss: 0.1238\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [39/50], Avg Loss: 0.1338\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [40/50], Avg Loss: 0.1123\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [41/50], Avg Loss: 0.1219\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [42/50], Avg Loss: 0.1273\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [43/50], Avg Loss: 0.1358\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [44/50], Avg Loss: 0.1572\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [45/50], Avg Loss: 0.1590\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [46/50], Avg Loss: 0.1046\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [47/50], Avg Loss: 0.1042\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [48/50], Avg Loss: 0.1403\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [49/50], Avg Loss: 0.2160\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch [50/50], Avg Loss: 0.1606\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "G4P5-5SiFqC9",
        "outputId": "3a970078-9dfd-4e14-8522-b230de1702c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNN(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 1000 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vYMZxVf3y-0",
        "outputId": "44bf596d-a192-4b70-a1fd-5ee8913990c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 1000 test images: 83.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIeyQFNgEIGc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applying this ANN in complete data**"
      ],
      "metadata": {
        "id": "QXd_ShBjXCDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(16)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=32,shuffle=False)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rXCN6kfEIB2",
        "outputId": "193f125e-c43d-4d64-8c6e-05a4af0771b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "├─Sequential: 1-1                        --\n",
            "│    └─Linear: 2-1                       100,480\n",
            "│    └─ReLU: 2-2                         --\n",
            "│    └─Linear: 2-3                       16,512\n",
            "│    └─ReLU: 2-4                         --\n",
            "│    └─Linear: 2-5                       1,290\n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/50], Loss: 0.6734\n",
            "=================================================================\n",
            "Epoch [2/50], Loss: 0.4695\n",
            "=================================================================\n",
            "Epoch [3/50], Loss: 0.4321\n",
            "=================================================================\n",
            "Epoch [4/50], Loss: 0.4095\n",
            "=================================================================\n",
            "Epoch [5/50], Loss: 0.3913\n",
            "=================================================================\n",
            "Epoch [6/50], Loss: 0.3820\n",
            "=================================================================\n",
            "Epoch [7/50], Loss: 0.3750\n",
            "=================================================================\n",
            "Epoch [8/50], Loss: 0.3658\n",
            "=================================================================\n",
            "Epoch [9/50], Loss: 0.3587\n",
            "=================================================================\n",
            "Epoch [10/50], Loss: 0.3543\n",
            "=================================================================\n",
            "Epoch [11/50], Loss: 0.3474\n",
            "=================================================================\n",
            "Epoch [12/50], Loss: 0.3414\n",
            "=================================================================\n",
            "Epoch [13/50], Loss: 0.3412\n",
            "=================================================================\n",
            "Epoch [14/50], Loss: 0.3396\n",
            "=================================================================\n",
            "Epoch [15/50], Loss: 0.3354\n",
            "=================================================================\n",
            "Epoch [16/50], Loss: 0.3383\n",
            "=================================================================\n",
            "Epoch [17/50], Loss: 0.3301\n",
            "=================================================================\n",
            "Epoch [18/50], Loss: 0.3280\n",
            "=================================================================\n",
            "Epoch [19/50], Loss: 0.3235\n",
            "=================================================================\n",
            "Epoch [20/50], Loss: 0.3317\n",
            "=================================================================\n",
            "Epoch [21/50], Loss: 0.3253\n",
            "=================================================================\n",
            "Epoch [22/50], Loss: 0.3209\n",
            "=================================================================\n",
            "Epoch [23/50], Loss: 0.3250\n",
            "=================================================================\n",
            "Epoch [24/50], Loss: 0.3201\n",
            "=================================================================\n",
            "Epoch [25/50], Loss: 0.3124\n",
            "=================================================================\n",
            "Epoch [26/50], Loss: 0.3135\n",
            "=================================================================\n",
            "Epoch [27/50], Loss: 0.3114\n",
            "=================================================================\n",
            "Epoch [28/50], Loss: 0.3115\n",
            "=================================================================\n",
            "Epoch [29/50], Loss: 0.3071\n",
            "=================================================================\n",
            "Epoch [30/50], Loss: 0.3131\n",
            "=================================================================\n",
            "Epoch [31/50], Loss: 0.3112\n",
            "=================================================================\n",
            "Epoch [32/50], Loss: 0.3133\n",
            "=================================================================\n",
            "Epoch [33/50], Loss: 0.3107\n",
            "=================================================================\n",
            "Epoch [34/50], Loss: 0.2997\n",
            "=================================================================\n",
            "Epoch [35/50], Loss: 0.3263\n",
            "=================================================================\n",
            "Epoch [36/50], Loss: 0.3222\n",
            "=================================================================\n",
            "Epoch [37/50], Loss: 0.3109\n",
            "=================================================================\n",
            "Epoch [38/50], Loss: 0.3102\n",
            "=================================================================\n",
            "Epoch [39/50], Loss: 0.3016\n",
            "=================================================================\n",
            "Epoch [40/50], Loss: 0.3055\n",
            "=================================================================\n",
            "Epoch [41/50], Loss: 0.2961\n",
            "=================================================================\n",
            "Epoch [42/50], Loss: 0.3022\n",
            "=================================================================\n",
            "Epoch [43/50], Loss: 0.3004\n",
            "=================================================================\n",
            "Epoch [44/50], Loss: 0.2953\n",
            "=================================================================\n",
            "Epoch [45/50], Loss: 0.3030\n",
            "=================================================================\n",
            "Epoch [46/50], Loss: 0.2984\n",
            "=================================================================\n",
            "Epoch [47/50], Loss: 0.3110\n",
            "=================================================================\n",
            "Epoch [48/50], Loss: 0.3012\n",
            "=================================================================\n",
            "Epoch [49/50], Loss: 0.2917\n",
            "=================================================================\n",
            "Epoch [50/50], Loss: 0.2942\n",
            "=================================================================\n",
            "Accuracy of the network on the test images: 86.36 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jm1JyInVEH8m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using GPU to Speedup Process**\n",
        "1. Check for gpu device availability.\n",
        "2. move model to gpu.\n",
        "3. use `pin_memory=True` parameter for faster data loading.\n",
        "4. move batches to gpu in both training and testing phase.\n",
        "5. use larger size batches."
      ],
      "metadata": {
        "id": "wNoi_b1WXNxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=256,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=256,shuffle=False,pin_memory=True)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 100\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "# move model to GPU\n",
        "model = model.to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = loss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "# checking accuracy in training data\n",
        "# checking if our model is overfitted\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the training images: {acc} %')\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the test images: {acc} %')"
      ],
      "metadata": {
        "id": "rrU304VjXNg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b105636-4f91-4993-b2f9-9cd01be8e75a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "├─Sequential: 1-1                        --\n",
            "│    └─Linear: 2-1                       100,480\n",
            "│    └─ReLU: 2-2                         --\n",
            "│    └─Linear: 2-3                       16,512\n",
            "│    └─ReLU: 2-4                         --\n",
            "│    └─Linear: 2-5                       1,290\n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/100], Avg Loss: 0.9130\n",
            "=================================================================\n",
            "Epoch [2/100], Avg Loss: 0.4426\n",
            "=================================================================\n",
            "Epoch [3/100], Avg Loss: 0.3908\n",
            "=================================================================\n",
            "Epoch [4/100], Avg Loss: 0.3624\n",
            "=================================================================\n",
            "Epoch [5/100], Avg Loss: 0.3423\n",
            "=================================================================\n",
            "Epoch [6/100], Avg Loss: 0.3339\n",
            "=================================================================\n",
            "Epoch [7/100], Avg Loss: 0.3168\n",
            "=================================================================\n",
            "Epoch [8/100], Avg Loss: 0.3188\n",
            "=================================================================\n",
            "Epoch [9/100], Avg Loss: 0.3109\n",
            "=================================================================\n",
            "Epoch [10/100], Avg Loss: 0.2953\n",
            "=================================================================\n",
            "Epoch [11/100], Avg Loss: 0.2920\n",
            "=================================================================\n",
            "Epoch [12/100], Avg Loss: 0.2829\n",
            "=================================================================\n",
            "Epoch [13/100], Avg Loss: 0.2831\n",
            "=================================================================\n",
            "Epoch [14/100], Avg Loss: 0.2787\n",
            "=================================================================\n",
            "Epoch [15/100], Avg Loss: 0.2679\n",
            "=================================================================\n",
            "Epoch [16/100], Avg Loss: 0.2728\n",
            "=================================================================\n",
            "Epoch [17/100], Avg Loss: 0.2606\n",
            "=================================================================\n",
            "Epoch [18/100], Avg Loss: 0.2606\n",
            "=================================================================\n",
            "Epoch [19/100], Avg Loss: 0.2550\n",
            "=================================================================\n",
            "Epoch [20/100], Avg Loss: 0.2506\n",
            "=================================================================\n",
            "Epoch [21/100], Avg Loss: 0.2456\n",
            "=================================================================\n",
            "Epoch [22/100], Avg Loss: 0.2458\n",
            "=================================================================\n",
            "Epoch [23/100], Avg Loss: 0.2370\n",
            "=================================================================\n",
            "Epoch [24/100], Avg Loss: 0.2340\n",
            "=================================================================\n",
            "Epoch [25/100], Avg Loss: 0.2336\n",
            "=================================================================\n",
            "Epoch [26/100], Avg Loss: 0.2346\n",
            "=================================================================\n",
            "Epoch [27/100], Avg Loss: 0.2339\n",
            "=================================================================\n",
            "Epoch [28/100], Avg Loss: 0.2330\n",
            "=================================================================\n",
            "Epoch [29/100], Avg Loss: 0.2259\n",
            "=================================================================\n",
            "Epoch [30/100], Avg Loss: 0.2184\n",
            "=================================================================\n",
            "Epoch [31/100], Avg Loss: 0.2175\n",
            "=================================================================\n",
            "Epoch [32/100], Avg Loss: 0.2210\n",
            "=================================================================\n",
            "Epoch [33/100], Avg Loss: 0.2260\n",
            "=================================================================\n",
            "Epoch [34/100], Avg Loss: 0.2204\n",
            "=================================================================\n",
            "Epoch [35/100], Avg Loss: 0.2212\n",
            "=================================================================\n",
            "Epoch [36/100], Avg Loss: 0.2156\n",
            "=================================================================\n",
            "Epoch [37/100], Avg Loss: 0.2119\n",
            "=================================================================\n",
            "Epoch [38/100], Avg Loss: 0.2088\n",
            "=================================================================\n",
            "Epoch [39/100], Avg Loss: 0.2024\n",
            "=================================================================\n",
            "Epoch [40/100], Avg Loss: 0.2047\n",
            "=================================================================\n",
            "Epoch [41/100], Avg Loss: 0.2048\n",
            "=================================================================\n",
            "Epoch [42/100], Avg Loss: 0.1945\n",
            "=================================================================\n",
            "Epoch [43/100], Avg Loss: 0.1993\n",
            "=================================================================\n",
            "Epoch [44/100], Avg Loss: 0.1937\n",
            "=================================================================\n",
            "Epoch [45/100], Avg Loss: 0.1919\n",
            "=================================================================\n",
            "Epoch [46/100], Avg Loss: 0.1972\n",
            "=================================================================\n",
            "Epoch [47/100], Avg Loss: 0.1870\n",
            "=================================================================\n",
            "Epoch [48/100], Avg Loss: 0.1969\n",
            "=================================================================\n",
            "Epoch [49/100], Avg Loss: 0.1907\n",
            "=================================================================\n",
            "Epoch [50/100], Avg Loss: 0.1796\n",
            "=================================================================\n",
            "Epoch [51/100], Avg Loss: 0.1810\n",
            "=================================================================\n",
            "Epoch [52/100], Avg Loss: 0.1879\n",
            "=================================================================\n",
            "Epoch [53/100], Avg Loss: 0.1885\n",
            "=================================================================\n",
            "Epoch [54/100], Avg Loss: 0.1852\n",
            "=================================================================\n",
            "Epoch [55/100], Avg Loss: 0.1762\n",
            "=================================================================\n",
            "Epoch [56/100], Avg Loss: 0.1812\n",
            "=================================================================\n",
            "Epoch [57/100], Avg Loss: 0.1735\n",
            "=================================================================\n",
            "Epoch [58/100], Avg Loss: 0.1729\n",
            "=================================================================\n",
            "Epoch [59/100], Avg Loss: 0.1715\n",
            "=================================================================\n",
            "Epoch [60/100], Avg Loss: 0.1741\n",
            "=================================================================\n",
            "Epoch [61/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [62/100], Avg Loss: 0.1778\n",
            "=================================================================\n",
            "Epoch [63/100], Avg Loss: 0.1700\n",
            "=================================================================\n",
            "Epoch [64/100], Avg Loss: 0.1604\n",
            "=================================================================\n",
            "Epoch [65/100], Avg Loss: 0.1690\n",
            "=================================================================\n",
            "Epoch [66/100], Avg Loss: 0.1771\n",
            "=================================================================\n",
            "Epoch [67/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [68/100], Avg Loss: 0.1731\n",
            "=================================================================\n",
            "Epoch [69/100], Avg Loss: 0.1613\n",
            "=================================================================\n",
            "Epoch [70/100], Avg Loss: 0.1606\n",
            "=================================================================\n",
            "Epoch [71/100], Avg Loss: 0.1675\n",
            "=================================================================\n",
            "Epoch [72/100], Avg Loss: 0.1583\n",
            "=================================================================\n",
            "Epoch [73/100], Avg Loss: 0.1507\n",
            "=================================================================\n",
            "Epoch [74/100], Avg Loss: 0.1562\n",
            "=================================================================\n",
            "Epoch [75/100], Avg Loss: 0.1658\n",
            "=================================================================\n",
            "Epoch [76/100], Avg Loss: 0.1575\n",
            "=================================================================\n",
            "Epoch [77/100], Avg Loss: 0.1554\n",
            "=================================================================\n",
            "Epoch [78/100], Avg Loss: 0.1508\n",
            "=================================================================\n",
            "Epoch [79/100], Avg Loss: 0.1509\n",
            "=================================================================\n",
            "Epoch [80/100], Avg Loss: 0.1538\n",
            "=================================================================\n",
            "Epoch [81/100], Avg Loss: 0.1479\n",
            "=================================================================\n",
            "Epoch [82/100], Avg Loss: 0.1567\n",
            "=================================================================\n",
            "Epoch [83/100], Avg Loss: 0.1545\n",
            "=================================================================\n",
            "Epoch [84/100], Avg Loss: 0.1617\n",
            "=================================================================\n",
            "Epoch [85/100], Avg Loss: 0.1491\n",
            "=================================================================\n",
            "Epoch [86/100], Avg Loss: 0.1380\n",
            "=================================================================\n",
            "Epoch [87/100], Avg Loss: 0.1453\n",
            "=================================================================\n",
            "Epoch [88/100], Avg Loss: 0.1424\n",
            "=================================================================\n",
            "Epoch [89/100], Avg Loss: 0.1557\n",
            "=================================================================\n",
            "Epoch [90/100], Avg Loss: 0.1356\n",
            "=================================================================\n",
            "Epoch [91/100], Avg Loss: 0.1396\n",
            "=================================================================\n",
            "Epoch [92/100], Avg Loss: 0.1492\n",
            "=================================================================\n",
            "Epoch [93/100], Avg Loss: 0.1480\n",
            "=================================================================\n",
            "Epoch [94/100], Avg Loss: 0.1498\n",
            "=================================================================\n",
            "Epoch [95/100], Avg Loss: 0.1364\n",
            "=================================================================\n",
            "Epoch [96/100], Avg Loss: 0.1431\n",
            "=================================================================\n",
            "Epoch [97/100], Avg Loss: 0.1373\n",
            "=================================================================\n",
            "Epoch [98/100], Avg Loss: 0.1315\n",
            "=================================================================\n",
            "Epoch [99/100], Avg Loss: 0.1224\n",
            "=================================================================\n",
            "Epoch [100/100], Avg Loss: 0.1489\n",
            "=================================================================\n",
            "Accuracy of the network on the training images: 95.01 %\n",
            "Accuracy of the network on the test images: 87.85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizing model performance**\n",
        "- Dropout using `p=0.3`\n",
        "- Batch Normalization using `BatchNorm1D`\n",
        "- Early stopping\n",
        "- Regularization using `weight_decay`\n",
        "- Adding more data\n",
        "- Reducing Network complexity\n",
        "- Data augmentation"
      ],
      "metadata": {
        "id": "9wyo218zCczJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=256,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=256,shuffle=False,pin_memory=True)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Linear(input_size,hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size,output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train_t.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10  # 10 classes for Fashion MNIST\n",
        "num_epochs = 100\n",
        "lr = 0.001\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "print(summary(model))\n",
        "\n",
        "# Training (one batch of dataset per epoch)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss=0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        # forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # loss calculation\n",
        "        l = nn.CrossEntropyLoss(outputs, batch_y.squeeze(1))\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        # updating grads\n",
        "        optimizer.step()\n",
        "        total_loss+=l.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "# checking accuracy in training data\n",
        "# checking if our model is overfitted\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the training images: {acc} %')\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# testing accuracy\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # move batches to GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += batch_y.size(0)\n",
        "        n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the testing images: {acc} %')"
      ],
      "metadata": {
        "id": "PbXzdrV3C_Od",
        "outputId": "159c8fb1-f003-4f73-be64-fe231f9bc1d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "SimpleNN                                 --\n",
            "├─Sequential: 1-1                        --\n",
            "│    └─Linear: 2-1                       100,480\n",
            "│    └─BatchNorm1d: 2-2                  256\n",
            "│    └─ReLU: 2-3                         --\n",
            "│    └─Dropout: 2-4                      --\n",
            "│    └─Linear: 2-5                       16,512\n",
            "│    └─BatchNorm1d: 2-6                  256\n",
            "│    └─ReLU: 2-7                         --\n",
            "│    └─Dropout: 2-8                      --\n",
            "│    └─Linear: 2-9                       1,290\n",
            "=================================================================\n",
            "Total params: 118,794\n",
            "Trainable params: 118,794\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "Epoch [1/100], Avg Loss: 0.6251\n",
            "=================================================================\n",
            "Epoch [2/100], Avg Loss: 0.4244\n",
            "=================================================================\n",
            "Epoch [3/100], Avg Loss: 0.3863\n",
            "=================================================================\n",
            "Epoch [4/100], Avg Loss: 0.3614\n",
            "=================================================================\n",
            "Epoch [5/100], Avg Loss: 0.3419\n",
            "=================================================================\n",
            "Epoch [6/100], Avg Loss: 0.3316\n",
            "=================================================================\n",
            "Epoch [7/100], Avg Loss: 0.3208\n",
            "=================================================================\n",
            "Epoch [8/100], Avg Loss: 0.3135\n",
            "=================================================================\n",
            "Epoch [9/100], Avg Loss: 0.3056\n",
            "=================================================================\n",
            "Epoch [10/100], Avg Loss: 0.2982\n",
            "=================================================================\n",
            "Epoch [11/100], Avg Loss: 0.2918\n",
            "=================================================================\n",
            "Epoch [12/100], Avg Loss: 0.2797\n",
            "=================================================================\n",
            "Epoch [13/100], Avg Loss: 0.2777\n",
            "=================================================================\n",
            "Epoch [14/100], Avg Loss: 0.2730\n",
            "=================================================================\n",
            "Epoch [15/100], Avg Loss: 0.2688\n",
            "=================================================================\n",
            "Epoch [16/100], Avg Loss: 0.2624\n",
            "=================================================================\n",
            "Epoch [17/100], Avg Loss: 0.2616\n",
            "=================================================================\n",
            "Epoch [18/100], Avg Loss: 0.2577\n",
            "=================================================================\n",
            "Epoch [19/100], Avg Loss: 0.2526\n",
            "=================================================================\n",
            "Epoch [20/100], Avg Loss: 0.2454\n",
            "=================================================================\n",
            "Epoch [21/100], Avg Loss: 0.2488\n",
            "=================================================================\n",
            "Epoch [22/100], Avg Loss: 0.2449\n",
            "=================================================================\n",
            "Epoch [23/100], Avg Loss: 0.2379\n",
            "=================================================================\n",
            "Epoch [24/100], Avg Loss: 0.2365\n",
            "=================================================================\n",
            "Epoch [25/100], Avg Loss: 0.2336\n",
            "=================================================================\n",
            "Epoch [26/100], Avg Loss: 0.2320\n",
            "=================================================================\n",
            "Epoch [27/100], Avg Loss: 0.2322\n",
            "=================================================================\n",
            "Epoch [28/100], Avg Loss: 0.2277\n",
            "=================================================================\n",
            "Epoch [29/100], Avg Loss: 0.2242\n",
            "=================================================================\n",
            "Epoch [30/100], Avg Loss: 0.2230\n",
            "=================================================================\n",
            "Epoch [31/100], Avg Loss: 0.2174\n",
            "=================================================================\n",
            "Epoch [32/100], Avg Loss: 0.2172\n",
            "=================================================================\n",
            "Epoch [33/100], Avg Loss: 0.2186\n",
            "=================================================================\n",
            "Epoch [34/100], Avg Loss: 0.2183\n",
            "=================================================================\n",
            "Epoch [35/100], Avg Loss: 0.2098\n",
            "=================================================================\n",
            "Epoch [36/100], Avg Loss: 0.2136\n",
            "=================================================================\n",
            "Epoch [37/100], Avg Loss: 0.2127\n",
            "=================================================================\n",
            "Epoch [38/100], Avg Loss: 0.2078\n",
            "=================================================================\n",
            "Epoch [39/100], Avg Loss: 0.2101\n",
            "=================================================================\n",
            "Epoch [40/100], Avg Loss: 0.2067\n",
            "=================================================================\n",
            "Epoch [41/100], Avg Loss: 0.2013\n",
            "=================================================================\n",
            "Epoch [42/100], Avg Loss: 0.2036\n",
            "=================================================================\n",
            "Epoch [43/100], Avg Loss: 0.2032\n",
            "=================================================================\n",
            "Epoch [44/100], Avg Loss: 0.2013\n",
            "=================================================================\n",
            "Epoch [45/100], Avg Loss: 0.2012\n",
            "=================================================================\n",
            "Epoch [46/100], Avg Loss: 0.1999\n",
            "=================================================================\n",
            "Epoch [47/100], Avg Loss: 0.1981\n",
            "=================================================================\n",
            "Epoch [48/100], Avg Loss: 0.1958\n",
            "=================================================================\n",
            "Epoch [49/100], Avg Loss: 0.1945\n",
            "=================================================================\n",
            "Epoch [50/100], Avg Loss: 0.1937\n",
            "=================================================================\n",
            "Epoch [51/100], Avg Loss: 0.1934\n",
            "=================================================================\n",
            "Epoch [52/100], Avg Loss: 0.1942\n",
            "=================================================================\n",
            "Epoch [53/100], Avg Loss: 0.1898\n",
            "=================================================================\n",
            "Epoch [54/100], Avg Loss: 0.1872\n",
            "=================================================================\n",
            "Epoch [55/100], Avg Loss: 0.1894\n",
            "=================================================================\n",
            "Epoch [56/100], Avg Loss: 0.1867\n",
            "=================================================================\n",
            "Epoch [57/100], Avg Loss: 0.1899\n",
            "=================================================================\n",
            "Epoch [58/100], Avg Loss: 0.1864\n",
            "=================================================================\n",
            "Epoch [59/100], Avg Loss: 0.1831\n",
            "=================================================================\n",
            "Epoch [60/100], Avg Loss: 0.1853\n",
            "=================================================================\n",
            "Epoch [61/100], Avg Loss: 0.1830\n",
            "=================================================================\n",
            "Epoch [62/100], Avg Loss: 0.1854\n",
            "=================================================================\n",
            "Epoch [63/100], Avg Loss: 0.1810\n",
            "=================================================================\n",
            "Epoch [64/100], Avg Loss: 0.1811\n",
            "=================================================================\n",
            "Epoch [65/100], Avg Loss: 0.1824\n",
            "=================================================================\n",
            "Epoch [66/100], Avg Loss: 0.1803\n",
            "=================================================================\n",
            "Epoch [67/100], Avg Loss: 0.1767\n",
            "=================================================================\n",
            "Epoch [68/100], Avg Loss: 0.1774\n",
            "=================================================================\n",
            "Epoch [69/100], Avg Loss: 0.1764\n",
            "=================================================================\n",
            "Epoch [70/100], Avg Loss: 0.1755\n",
            "=================================================================\n",
            "Epoch [71/100], Avg Loss: 0.1763\n",
            "=================================================================\n",
            "Epoch [72/100], Avg Loss: 0.1745\n",
            "=================================================================\n",
            "Epoch [73/100], Avg Loss: 0.1761\n",
            "=================================================================\n",
            "Epoch [74/100], Avg Loss: 0.1734\n",
            "=================================================================\n",
            "Epoch [75/100], Avg Loss: 0.1759\n",
            "=================================================================\n",
            "Epoch [76/100], Avg Loss: 0.1752\n",
            "=================================================================\n",
            "Epoch [77/100], Avg Loss: 0.1733\n",
            "=================================================================\n",
            "Epoch [78/100], Avg Loss: 0.1719\n",
            "=================================================================\n",
            "Epoch [79/100], Avg Loss: 0.1705\n",
            "=================================================================\n",
            "Epoch [80/100], Avg Loss: 0.1735\n",
            "=================================================================\n",
            "Epoch [81/100], Avg Loss: 0.1688\n",
            "=================================================================\n",
            "Epoch [82/100], Avg Loss: 0.1680\n",
            "=================================================================\n",
            "Epoch [83/100], Avg Loss: 0.1687\n",
            "=================================================================\n",
            "Epoch [84/100], Avg Loss: 0.1678\n",
            "=================================================================\n",
            "Epoch [85/100], Avg Loss: 0.1665\n",
            "=================================================================\n",
            "Epoch [86/100], Avg Loss: 0.1680\n",
            "=================================================================\n",
            "Epoch [87/100], Avg Loss: 0.1652\n",
            "=================================================================\n",
            "Epoch [88/100], Avg Loss: 0.1698\n",
            "=================================================================\n",
            "Epoch [89/100], Avg Loss: 0.1654\n",
            "=================================================================\n",
            "Epoch [90/100], Avg Loss: 0.1639\n",
            "=================================================================\n",
            "Epoch [91/100], Avg Loss: 0.1687\n",
            "=================================================================\n",
            "Epoch [92/100], Avg Loss: 0.1656\n",
            "=================================================================\n",
            "Epoch [93/100], Avg Loss: 0.1652\n",
            "=================================================================\n",
            "Epoch [94/100], Avg Loss: 0.1625\n",
            "=================================================================\n",
            "Epoch [95/100], Avg Loss: 0.1656\n",
            "=================================================================\n",
            "Epoch [96/100], Avg Loss: 0.1638\n",
            "=================================================================\n",
            "Epoch [97/100], Avg Loss: 0.1650\n",
            "=================================================================\n",
            "Epoch [98/100], Avg Loss: 0.1658\n",
            "=================================================================\n",
            "Epoch [99/100], Avg Loss: 0.1603\n",
            "=================================================================\n",
            "Epoch [100/100], Avg Loss: 0.1614\n",
            "=================================================================\n",
            "Accuracy of the network on the training images: 94.20166666666667 %\n",
            "Accuracy of the network on the testing images: 89.26 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameter tuning with Optuna**\n",
        "- no of hidden layer\n",
        "- no of neurons per layer\n",
        "- no of epochs\n",
        "- optimizer\n",
        "- learning rate\n",
        "- batch size\n",
        "- dropout\n",
        "- weight decay"
      ],
      "metadata": {
        "id": "h9U8cjETIUMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "bkxf3fZCGVCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2b99c3-bb2f-48e5-b706-2c5bbca124a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "train_loader=DataLoader(dataset=train_dataset,batch_size=256,shuffle=True,pin_memory=True)\n",
        "test_loader=DataLoader(dataset=test_dataset,batch_size=256,shuffle=False,pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky9Qz38mI8PY",
        "outputId": "90b60a41-1ed2-4970-9a19-1b8e8d6dc60a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "class myNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, neurons_per_layer, droput_rate):\n",
        "        super(myNN, self).__init__()\n",
        "        layers=[]\n",
        "        for i in range(hidden_size):\n",
        "            layers.append(nn.Linear(input_size,neurons_per_layer))\n",
        "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(droput_rate))\n",
        "            input_size=neurons_per_layer\n",
        "        layers.append(nn.Linear(neurons_per_layer,output_size))\n",
        "        self.model=nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "q51tTqSyNUOY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# define objective fn\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    hidden_size = trial.suggest_int('hidden_size', low=2, high=24)\n",
        "    neurons_per_layer = trial.suggest_int('neurons_per_layer', low=8, high=256, step=8)\n",
        "    input_size=784\n",
        "    output_size=10\n",
        "    output_size = 10  # 10 classes for Fashion MNIST\n",
        "    num_epochs = 100\n",
        "    lr = 0.001\n",
        "\n",
        "    # define network\n",
        "    model=myNN(input_size,hidden_size,output_size,neurons_per_layer)\n",
        "    model=model.to(device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    # loss\n",
        "    loss=nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training (one batch of dataset per epoch)\n",
        "    for epoch in range(num_epochs):\n",
        "      for batch_X, batch_y in train_loader:\n",
        "          # move batches to GPU\n",
        "          batch_X = batch_X.to(device)\n",
        "          batch_y = batch_y.to(device)\n",
        "          # forward pass\n",
        "          outputs = model(batch_X)\n",
        "          # loss calculation\n",
        "          l = loss(outputs, batch_y.squeeze(1))\n",
        "          # backward pass\n",
        "          optimizer.zero_grad()\n",
        "          l.backward()\n",
        "          # updating grads\n",
        "          optimizer.step()\n",
        "\n",
        "    # model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            # move batches to GPU\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            # max returns (value ,index)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            n_samples += batch_y.size(0)\n",
        "            n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "\n",
        "    return acc  # Return the accuracy score for Optuna to maximize\n",
        "\n",
        "\n",
        "# study object\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())  # We aim to maximize accuracy\n",
        "# start study\n",
        "study.optimize(objective, n_trials=10)  # Run 50 trials to find the best hyperparameters\n",
        "# check best trial and parameters\n",
        "print(f'Best trial accuracy: {study.best_trial.value}')\n",
        "print(f'Best hyperparameters: {study.best_trial.params}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPfiTU7KNULO",
        "outputId": "bed59beb-050c-42af-ab93-5e4f869602b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-20 09:20:44,345] A new study created in memory with name: no-name-4e962b01-5158-476b-826f-8d7ce1cb18ff\n",
            "[I 2025-09-20 09:24:30,543] Trial 0 finished with value: 88.87 and parameters: {'hidden_size': 14, 'neurons_per_layer': 248}. Best is trial 0 with value: 88.87.\n",
            "[I 2025-09-20 09:26:58,957] Trial 1 finished with value: 88.83 and parameters: {'hidden_size': 6, 'neurons_per_layer': 192}. Best is trial 0 with value: 88.87.\n",
            "[I 2025-09-20 09:30:22,023] Trial 2 finished with value: 89.09 and parameters: {'hidden_size': 11, 'neurons_per_layer': 200}. Best is trial 2 with value: 89.09.\n",
            "[I 2025-09-20 09:34:50,740] Trial 3 finished with value: 88.96 and parameters: {'hidden_size': 17, 'neurons_per_layer': 256}. Best is trial 2 with value: 89.09.\n",
            "[I 2025-09-20 09:38:59,715] Trial 4 finished with value: 46.63 and parameters: {'hidden_size': 17, 'neurons_per_layer': 56}. Best is trial 2 with value: 89.09.\n",
            "[I 2025-09-20 09:41:13,673] Trial 5 finished with value: 89.55 and parameters: {'hidden_size': 6, 'neurons_per_layer': 208}. Best is trial 5 with value: 89.55.\n",
            "[I 2025-09-20 09:46:53,222] Trial 6 finished with value: 57.04 and parameters: {'hidden_size': 23, 'neurons_per_layer': 152}. Best is trial 5 with value: 89.55.\n",
            "[I 2025-09-20 09:50:21,934] Trial 7 finished with value: 37.87 and parameters: {'hidden_size': 13, 'neurons_per_layer': 32}. Best is trial 5 with value: 89.55.\n",
            "[I 2025-09-20 09:55:29,324] Trial 8 finished with value: 47.08 and parameters: {'hidden_size': 23, 'neurons_per_layer': 152}. Best is trial 5 with value: 89.55.\n",
            "[I 2025-09-20 09:59:25,890] Trial 9 finished with value: 75.11 and parameters: {'hidden_size': 16, 'neurons_per_layer': 80}. Best is trial 5 with value: 89.55.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial accuracy: 89.55\n",
            "Best hyperparameters: {'hidden_size': 6, 'neurons_per_layer': 208}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from keras import datasets\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# check gpu availability\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "else:\n",
        "  device=torch.device(\"cpu\")\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) =datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Reshape the arrays to be 2-dimensional\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# convert into Dataframe\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# convert into tensors\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# dataset and dataloader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.n_samples=X.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "train_dataset=custom_dataset(X_train_t,y_train_t)\n",
        "test_dataset=custom_dataset(X_test_t,y_test_t)\n",
        "\n",
        "\n",
        "# define neural network\n",
        "class myNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, neurons_per_layer, droput_rate):\n",
        "        super(myNN, self).__init__()\n",
        "        layers=[]\n",
        "        for i in range(hidden_size):\n",
        "            layers.append(nn.Linear(input_size,neurons_per_layer))\n",
        "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(droput_rate))\n",
        "            input_size=neurons_per_layer\n",
        "        layers.append(nn.Linear(neurons_per_layer,output_size))\n",
        "        self.model=nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# define objective fn\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    hidden_size = trial.suggest_int('hidden_size', low=2, high=24)\n",
        "    neurons_per_layer = trial.suggest_int('neurons_per_layer', low=8, high=256, step=8)\n",
        "    input_size=784 # 784 pixels in image\n",
        "    output_size=10 # 10 classes for Fashion MNIST\n",
        "    num_epochs = trial.suggest_int('num_epochs', low=10, high=100,step=10)\n",
        "    lr = trial.suggest_float('lr', low=1e-5, high=1e-1, log=True)\n",
        "    batch_size=trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "    droput_rate=trial.suggest_float('droput_rate', low=0.2, high=0.5, step=0.1)\n",
        "    weight_decay=trial.suggest_float('weight_decay', low=1e-5, high=1e-1, log=True)\n",
        "    optimizer_name=trial.suggest_categorical('optimizer', ['Adam', 'SGD','RMSprop'])\n",
        "\n",
        "    # datloader with trial batch size\n",
        "    train_loader=DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
        "    test_loader=DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
        "\n",
        "    # define network\n",
        "    model=myNN(input_size,hidden_size,output_size,neurons_per_layer,droput_rate)\n",
        "    model=model.to(device)\n",
        "\n",
        "    # optimizer\n",
        "    if(optimizer_name==\"Adam\"):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif(optimizer_name==\"SGD\"):\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # loss\n",
        "    loss=nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training (one batch of dataset per epoch)\n",
        "    for epoch in range(num_epochs):\n",
        "      for batch_X, batch_y in train_loader:\n",
        "          # move batches to GPU\n",
        "          batch_X = batch_X.to(device)\n",
        "          batch_y = batch_y.to(device)\n",
        "          # forward pass\n",
        "          outputs = model(batch_X)\n",
        "          # loss calculation\n",
        "          l = loss(outputs, batch_y.squeeze(1))\n",
        "          # backward pass\n",
        "          optimizer.zero_grad()\n",
        "          l.backward()\n",
        "          # updating grads\n",
        "          optimizer.step()\n",
        "\n",
        "    # model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            # move batches to GPU\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            # max returns (value ,index)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            n_samples += batch_y.size(0)\n",
        "            n_correct += (predicted == batch_y.squeeze(1)).sum().item()\n",
        "\n",
        "        acc = n_correct / n_samples\n",
        "\n",
        "    return acc  # Return the accuracy score for Optuna to maximize\n",
        "\n",
        "\n",
        "# study object\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())  # We aim to maximize accuracy\n",
        "# start study\n",
        "study.optimize(objective, n_trials=20)  # Run 50 trials to find the best hyperparameters\n",
        "# check best trial and parameters\n",
        "print(f'Best trial accuracy: {study.best_trial.value}')\n",
        "print(f'Best hyperparameters: {study.best_trial.params}')\n"
      ],
      "metadata": {
        "id": "DyombUYuNUFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEZwN8ZCNUB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}